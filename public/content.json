{"pages":[{"title":"about","text":"YC Lin","link":"/about/index.html"}],"posts":[{"title":"Emergency - 911 Calls 資料分析","text":"Dataset : Kaggle - Emergency - 911 Calls / Montgomery County, PA 使用Python套件 數據分析 NumPy Pandas 資料視覺化 matplotlib Seaborn 1234import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns 讀取dataset12df = pd.read_csv('911.csv')df.info() 1df.head() 撥打911最多的5個郵遞區號12345678df['zip'].value_counts().head() # 19401.0 6979# 19464.0 6643# 19403.0 4854# 19446.0 4748# 19406.0 3174# Name: zip, dtype: int64 撥打911最多的5個鄉鎮12345678df['twp'].value_counts().head()# LOWER MERION 8443# ABINGTON 5977# NORRISTOWN 5890# UPPER MERION 5227# CHELTENHAM 4575# Name: twp, dtype: int64 title欄有幾筆唯一的資料1234# len(df['title'].unique()) df['title'].nunique()# 110 分析title欄的類別，新增Reason欄位1234567#先取出類別x = df['title'].iloc[0]x.split(':')[0] # 'EMS'# 新增Reasons欄位df['Reason'] = df['title'].apply(lambda title: title.split(':')[0])df 各種Reason案件數量123456df['Reason'].value_counts()# EMS 48877# Traffic 35695# Fire 14920# Name: Reason, dtype: int64 使用Seaborn產生圖表 1sns.countplot(x = 'Reason', data = df, palette = 'viridis') 分析timeStamp，新增Hour、Month、Day of Week欄位 使用pandas的to_datetime轉成Date Time物件 123456789101112type(df['timeStamp'].iloc[0]) # strdf['timeStamp'] = pd.to_datetime(df['timeStamp'])type(df['timeStamp'].iloc[0]) # pandas._libs.tslibs.timestamps.Timestamptime = df['timeStamp'].iloc[0]time.hour # 取出hour值# 新增欄位df['Hour'] = df['timeStamp'].apply(lambda time: time.hour)df['Month'] = df['timeStamp'].apply(lambda time: time.month)df['Day of Week'] = df['timeStamp'].apply(lambda time: time.dayofweek) 重新mapping星期幾的欄位 123dmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'}df['Day of Week'] = df['Day of Week'].map(dmap)df 用Seaborn產生橫軸為Day of Week計量圖表 12sns.countplot(x = 'Day of Week', data = df, hue = 'Reason', palette = 'viridis') plt.legend(bbox_to_anchor= (1.05, 1), loc = 2, borderaxespad = 0.)# 把legend放外面避免重疊 Month為橫軸 12sns.countplot(x = 'Month', data = df, hue = 'Reason', palette = 'viridis') plt.legend(bbox_to_anchor= (1.05, 1), loc = 2, borderaxespad = 0.) Day of Week對Hour的Heat map 需使用groupby與unstack重構dataFrame 1dayHour = df.groupby(by = ['Day of Week', 'Hour']).count()['Reason'].unstack() 用新的dataFrame產生Heat map 12plt.figure(figsize=(12,6))sns.heatmap(dayHour, cmap = 'viridis')","link":"/2018/10/11/ML-course-note-DataAnalysis-Emergency911/"},{"title":"Decision Tree and Random Forest - Lending Club 信貸資料分析","text":"Dataset Download各column所代表的意義如下 credit.policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise. purpose: The purpose of the loan (takes values “credit_card”, “debt_consolidation”, “educational”, “major_purchase”, “small_business”, and “all_other”). int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates. installment: The monthly installments owed by the borrower if the loan is funded. log.annual.inc: The natural log of the self-reported annual income of the borrower. dti: The debt-to-income ratio of the borrower (amount of debt divided by annual income). fico: The FICO credit score of the borrower. days.with.cr.line: The number of days the borrower has had a credit line. revol.bal: The borrower’s revolving balance (amount unpaid at the end of the credit card billing cycle). revol.util: The borrower’s revolving line utilization rate (the amount of the credit line used relative to total credit available). inq.last.6mths: The borrower’s number of inquiries by creditors in the last 6 months. delinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years. pub.rec: The borrower’s number of derogatory public records (bankruptcy filings, tax liens, or judgments). 函式庫1234import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns 引入資料集12df = pd.read_csv('loan_data.csv')df.head() 1df.info() RangeIndex: 9578 entries, 0 to 9577Data columns (total 14 columns):credit.policy 9578 non-null int64purpose 9578 non-null objectint.rate 9578 non-null float64installment 9578 non-null float64log.annual.inc 9578 non-null float64dti 9578 non-null float64fico 9578 non-null int64days.with.cr.line 9578 non-null float64revol.bal 9578 non-null int64revol.util 9578 non-null float64inq.last.6mths 9578 non-null int64delinq.2yrs 9578 non-null int64pub.rec 9578 non-null int64not.fully.paid 9578 non-null int64dtypes: float64(6), int64(7), object(1)memory usage: 1.0+ MB 1df.describe() 資料分析個別顯示credit.policy為1跟0的fico值分布之histogram 12345plt.figure(figsize=(10, 6))df[ df['credit.policy'] == 1 ]['fico'].hist(bins= 35, color = 'blue', label = 'Credit Policy = 1', alpha = 0.6)df[ df['credit.policy'] == 0 ]['fico'].hist(bins= 35, color = 'red', label = 'Credit Policy = 0', alpha = 0.6)plt.legend()plt.title('FICO') 同上，改成分為not.fully.paid為1、0的histogram 12345plt.figure(figsize=(10, 6))df[ df['not.fully.paid'] == 1 ]['fico'].hist(bins= 35, color = 'blue', label = 'not.fully.paid = 1', alpha = 0.6)df[ df['not.fully.paid'] == 0 ]['fico'].hist(bins= 35, color = 'red', label = 'not.fully.paid = 0', alpha = 0.6)plt.legend()plt.title('FICO') 使用seaborn，產生countplot來顯示每種purpose對應的not.fully.paid為0、1的資訊 12plt.figure(figsize=(11, 7))sns.countplot(x = 'purpose', hue = 'not.fully.paid', data = df) 顯示fico值與int.rate之間的關係 1sns.jointplot(x = 'fico', y= 'int.rate', data = df) 12plt.figure(figsize=(11,7))sns.lmplot(y= 'int.rate', x= 'fico', data = loans, hue = 'credit.policy', col = 'not.fully.paid', fit_reg=False) 資料處理 將類別變數改成dummy variable 123cat_feats = ['purpose'] #紀錄類別column名稱final_data = pd.get_dummies(df, columns=cat_feats, drop_first=True) #drop_first = True會丟棄第一行(但仍然能夠藉由其他行推斷類別)，減少不必要的資料 切分訓練、測試資料123456from sklearn.model_selection import train_test_splitX = final_data.drop('not.fully.paid', axis = 1)y = final_data['not.fully.paid']X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) 訓練Decision Tree model1234from sklearn.tree import DecisionTreeClassifierdTree = DecisionTreeClassifier()dTree.fit(X_train, y_train) 預測與評估1234567891011121314151617181920from sklearn.metrics import classification_report, confusion_matrixpredictions = dTree.predict(X_test)print(confusion_matrix(y_test, predictions))'''[[2006 395] [ 357 116]]'''print(classification_report(y_test, predictions))''' precision recall f1-score support 0 0.85 0.84 0.84 2401 1 0.23 0.25 0.24 473avg / total 0.75 0.74 0.74 2874''' 訓練Random Forest model1234from sklearn.ensemble import RandomForestClassifierrfc = RandomForestClassifier(n_estimators= 300)rfc.fit(X_train, y_train) 預測與評估1234567891011121314151617181920from sklearn.metrics import classification_report, confusion_matrixpredictions = dTree.predict(X_test)print(confusion_matrix(y_test, predictions))'''[[2363 38] [ 447 26]]'''print(classification_report(y_test, predictions))''' precision recall f1-score support 0 0.84 0.98 0.91 2401 1 0.41 0.05 0.10 473avg / total 0.77 0.83 0.77 2874'''","link":"/2018/10/11/ML-course-note-DecisionTree-RandomForest/"},{"title":"K-Means-Clustering演算法 - 腫瘤分類","text":"非監督式機器學習演算法，本文將依照資料集的features來分類良性或惡性腫瘤(乳癌)並用cufflinks作圖幫助我們做初步的資料分析dataset來自scikit-learn 函式庫12345678910111213import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inlinefrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplotimport cufflinks as cf# For Notebooksinit_notebook_mode(connected=True)# For offline usecf.go_offline() scikit-learn資料集12345678910from sklearn.datasets import load_breast_cancerdata = load_breast_cancer()data.keys()# print(data.DESCR)df = pd.DataFrame(data.data, columns= data.feature_names)df['target'] = data.targetdf.head() cufflinks 互動式資料分析1df.iplot(kind='scatter',x='mean radius',y='mean texture',mode='markers',size=10, categories= 'target', xTitle='mean radius', yTitle='mean texture') 具有部分縮放功能可查看資料細節 KMeans演算法由scikit-learn的cluster引入12345from sklearn.cluster import KMeanskmeans = KMeans(n_clusters=2) #分成2類kmeans.fit(df.drop('target', axis=1)) #注意! 非監督式不需要Label 查看重心1kmeans.cluster_centers_ array([[1.25562991e+01, 1.85703653e+01, 8.11234703e+01, 4.96061872e+02, 9.48844977e-02, 9.10998174e-02, 6.24377642e-02, 3.34325434e-02, 1.78057991e-01, 6.34540183e-02, 3.04190868e-01, 1.21515320e+00, 2.15288059e+00, 2.37852922e+01, 7.17326256e-03, 2.34746895e-02, 2.87455128e-02, 1.06363242e-02, 2.06135799e-02, 3.74750297e-03, 1.40439018e+01, 2.47095434e+01, 9.19375114e+01, 6.19647945e+02, 1.29959110e-01, 2.23311758e-01, 2.19214947e-01, 9.13298425e-02, 2.83553653e-01, 8.32819406e-02], [1.93799237e+01, 2.16945802e+01, 1.28231298e+02, 1.18592977e+03, 1.01294580e-01, 1.48612977e-01, 1.76939466e-01, 1.00698779e-01, 1.91539695e-01, 6.06029008e-02, 7.42803817e-01, 1.22253817e+00, 5.25058015e+00, 9.56781679e+01, 6.59868702e-03, 3.21766947e-02, 4.24197710e-02, 1.56739847e-02, 2.03039695e-02, 3.95338931e-03, 2.37094656e+01, 2.89126718e+01, 1.58496183e+02, 1.75302290e+03, 1.40424733e-01, 3.57757710e-01, 4.49306107e-01, 1.92431069e-01, 3.11881679e-01, 8.61654962e-02]]) 分群結果1kmeans.labels_ array([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1]) 視覺化比較分類結果12345f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(10,5))ax1.set_title('K Means')ax1.scatter(df['mean radius'], df['mean texture'], c = kmeans.labels_)ax2.set_title('Original')ax2.scatter(df['mean radius'], df['mean texture'], c = df['target']) 很明顯良性惡性的結果label反了(因為n_clusters=2，所以類型非0即1)，所以修改預測數值 (此步驟不一定要做) 12func = lambda x : 1 if x ==0 else 0predictions = np.array([func(x) for x in kmeans.labels_]) 再次作圖 12345f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(10,5))ax1.set_title('K Means')ax1.scatter(df['mean radius'], df['mean texture'], c = predictions)ax2.set_title('Original')ax2.scatter(df['mean radius'], df['mean texture'], c = df['target']) 評估預測準確度 123456789101112131415161718from sklearn.metrics import classification_report, confusion_matrixprint(confusion_matrix(df['target'], predictions))\"\"\" [[130 82] [ 1 356]]\"\"\" print(classification_report(df['target'], predictions))\"\"\" precision recall f1-score support 0 0.99 0.61 0.76 212 1 0.81 1.00 0.90 357avg / total 0.88 0.85 0.84 569\"\"\"","link":"/2018/10/12/ML-course-note-K-means/"},{"title":"Linear Regression - Boston house-prices dataset  房價預測模型","text":"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html 引入Libraries1234import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sns 讀取Dataset123from sklearn.datasets import load_bostonboston = load_boston()print(boston.DESCR) #查看說明文件 CRIM：城鎮人均犯罪率 ZN：佔地面積超過25,000平方英尺的住宅用地比例。 INDUS：每個城鎮的非零售業務面積比例 CHAS：Charles River虛擬變量（如果管道限制河流則= 1;否則為0） NOX：一氧化氮濃度（每千萬份） RM：每棟住宅的平均房間數 AGE：1940年以前建造的自住單位比例 DIS：到波士頓五個就業中心的加權距離 RAD：徑向高速公路的可達性指數 TAX：每10,000美元的全額房產稅率 PTRATIO：城鎮的學生與教師比例 B：1000（Bk - 0.63）^ 2其中Bk是城鎮黑人的比例 LSTAT：人口狀況下降％ MEDV：自住房屋的中位數價值(單位: USD1000) 整理dataFrame 1234df = pd.DataFrame(boston.data.T, ['CRIM','ZN','INDUS','CHAS','NOX','RM' ,'AGE','DIS','RAD','TAX', 'PTRATIO','B','LSTAT']) #有13個featuredf = df.Tdf['MEDV'] = boston.target.T #MEDV即預測目標向量df.head() 查看各項統計數據1df.describe() Exploratory data analysis1sns.pairplot(df) 對單一變量的數值分布 1sns.distplot(df['MEDV']) 查看各種feature間的相關性記得用corr()產生新的dataFrame 12plt.figure(figsize=(18, 12))sns.heatmap(df.corr(), annot= True, cmap = 'coolwarm') 訓練線性回歸模型載入sklearn123456from sklearn.model_selection import train_test_splitX = df[['CRIM','ZN','INDUS','CHAS','NOX','RM' ,'AGE','DIS','RAD','TAX', 'PTRATIO','B','LSTAT']]y = df['MEDV']#分出30%的資料作為test setX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) 使用LinearRegression建立並訓練模型123from sklearn.linear_model import LinearRegressionlm = LinearRegression()lm.fit(X_train,y_train) #Fit linear model 到這裡就完成模型的訓練，接著看看訓練成果 模型評估1lm.coef_ # 查看各項係數 表列係數 123coeff_df = pd.DataFrame(lm.coef_, ['CRIM','ZN','INDUS','CHAS','NOX','RM' ,'AGE','DIS','RAD','TAX', 'PTRATIO','B','LSTAT'], columns = ['Coefficient'])coeff_df['註記'] = ['城鎮人均犯罪率','佔地面積超過25,000平方英尺的住宅用地比例','每個城鎮的非零售業務面積比例','Charles River虛擬變量（如果管道限制河流則= 1;否則為0）','一氧化氮濃度（每千萬份）','每棟住宅的平均房間數','1940年以前建造的自住單位比例','到波士頓五個就業中心的加權距離','徑向高速公路的可達性指數','每10,000美元的全額房產稅率','城鎮的學生與教師比例','1000*(Bk - 0.63)^2，其中Bk是城鎮黑人的比例','人口狀況下降百分比']coeff_df coefficients代表feature每單位增加對房價(MEDV)的增量ex.平均房間數增加1，則房價(MEDV)會增加3.890080(單位:USD1000) 用建立的模型做預測1predictions = lm.predict(X_test) predictions即為預測房價，真實房價為y_test用scatter plot看預測正確性，高度正相關代表預測誤差越小 123plt.scatter(y_test,predictions) plt.xlabel('Measured')plt.ylabel('Predicted') 用distribution plot檢查預測與實際的差值，residual = y_test-predictions若residual位置集中在0的常態分布，則模型建立完成 1sns.distplot((y_test-predictions), bins=50); Regression Evaluation Metrics評估回歸模型Mean Absolute Error (MAE) is the mean of the absolute value of the errors: $$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$ Mean Squared Error (MSE) is the mean of the squared errors: $$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$ Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors: $$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$ 以上都是損失函數，越大代表與真實數據的不一致性越大，所以我們需要最小化他們才能做出正確預測 12345678from sklearn import metricsprint('MAE:', metrics.mean_absolute_error(y_test, predictions))print('MSE:', metrics.mean_squared_error(y_test, predictions))print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))# MAE: 82288.22251914957# MSE: 10460958907.209501# RMSE: 102278.82922291153","link":"/2018/10/11/ML-course-note-LinearRegression/"},{"title":"Logistic Regrassion - 廣告點擊資料分析&預測","text":"Dataset來源: 隨機產生的虛擬資料 Download目標是預測一個使用者是否有點擊廣告 引入函式庫1234import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns 讀取資料12ad_data = pd.read_csv('advertising.csv')ad_data.head() 欄位說明 ‘Daily Time Spent on Site’: consumer time on site in minutes ‘Age’: cutomer age in years ‘Area Income’: Avg. Income of geographical area of consumer ‘Daily Internet Usage’: Avg. minutes a day consumer is on the internet ‘Ad Topic Line’: Headline of the advertisement ‘City’: City of consumer ‘Male’: Whether or not consumer was male ‘Country’: Country of consumer ‘Timestamp’: Time at which consumer clicked on Ad or closed window ‘Clicked on Ad’: 0 or 1 indicated clicking on Ad 初步資料分析產生年齡的histgram12ad_data['Age'].plot.hist(bins = 30)plt.xlabel('Age') 產生Area Income對Age的joint plot1sns.jointplot(x = 'Age', y = 'Area Income', data = ad_data) 產生 Daily Time spent on site對Age的joint plot(以kde分布表示)1sns.jointplot(x = 'Age', y= 'Daily Time Spent on Site', data = ad_data, kind = 'kde') 產生hue設為’Clicked on Ad’的pairplot1sns.pairplot(ad_data, hue = 'Clicked on Ad') 訓練Logistic Regression 模型從dataset中切分出train data、test data 採用數值資料、’Ad Topic Line’這種字串無法學習，需要data clean將之去除，或者是想辦法轉換成dummy variable 123456from sklearn.model_selection import train_test_splitX = ad_data[['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage', 'Male']]y = ad_data['Clicked on Ad']X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101) 載入Logistic Regression模型123from sklearn.linear_model import LogisticRegressionlogmodel = LogisticRegression()logmodel.fit(X_train, y_train) 預測與評估輸入test set產生一組預測資料1predictions = logmodel.predict(X_test) 產生分類報告與confusion matrix接著拿出test set的y_test，即所謂實際結果，並用上面的predictions比較，計算出預測成功率 12from sklearn.metrics import classification_report, confusion_matrixprint(classification_report(y_test, predictions)) 1print(confusion_matrix(y_test, predictions)) &lt;img height = ‘180’ src = https://i.imgur.com/OYpeupc.png/&gt;","link":"/2018/10/11/ML-course-note-LogisticRegrassion/"},{"title":"K Nearest Neighbors 演算法","text":"基本概念如圖將Node分成A、B類，根據X1、X2兩種特徵標出待測物(星號標示)，重點在於k值選擇，若k = 3則相鄰B類Node數量較多，故預測其為B類，k = 6則相鄰A類Node較多，所以預測為A類 dataset來源: 隨機產生，KNN演算法的實行上不必知道各種feature是什麼Download 引入函式庫&amp;載入資料12345678import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsdf = pd.read_csv('KNN_Dataset')df.head() 初步資料分析1sns.pairplot(data = df, hue = 'TARGET CLASS') 標準化變數數值型變數標準化，避免ＫＮＮ距離計算時因為單位不同造成失真 可以使用Scikit learn內建標準化功能 123456789from sklearn.preprocessing import StandardScalerscaler = StandardScaler()scaler.fit(df.drop('TARGET CLASS', axis=1)) # 用.transform()方法回傳標準化後的featuresscaler_features = scaler.transform(df.drop('TARGET CLASS', axis=1))# 重新整理dataFramedf_feat = pd.DataFrame(scaler_features, columns=df.columns[:-1]) #columns和原本一樣df_feat.head() 拆分train data與test data123456from sklearn.model_selection import train_test_splitX = df_featy = df['TARGET CLASS']X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 隨機取30%的資料為test data 訓練KNN模型12345from sklearn.neighbors import KNeighborsClassifier#一開始先定k值為1，之後會再測試準確率最高的k值knn = KNeighborsClassifier(n_neighbors = 1)knn.fit(X_train, y_train) 預測與評估1234pred = knn.predict(X_test)from sklearn.metrics import classification_report, confusion_matrixprint(confusion_matrix(y_test, pred)) confusion matrix: [[109 43] [ 41 107]] 1print(classification_report(y_test, pred)) precision recall f1-score support 0 0.73 0.72 0.72 152 1 0.71 0.72 0.72 148 avg / total 0.72 0.72 0.72 300 選用適合的k值12345678910error_rate = []for i in range(1, 40): # range夠大即可，一般選1~40 knn = KNeighborsClassifier(n_neighbors=i) knn.fit(X_train, y_train) pred_i = knn.predict(X_test) #np.mean(pred_i != y_test)為預測失敗率 error_rate.append(np.mean(pred_i != y_test)) 圖表化k值對其預測失敗機率 12345plt.figure(figsize=(10, 6))plt.plot(range(1, 40), error_rate, color = 'blue', linestyle= '--', marker = 'o', markerfacecolor = 'red', markersize = 10)plt.title('Error Rate vs. K Value')plt.xlabel('K')plt.ylabel('Error Rate') 可看出k=30時失敗率最低(預測精確度最高)","link":"/2018/10/11/ML-course-note-KNN/"},{"title":"Raspberry Pi3 MJPEG-Streamer Install & Setup","text":"材料 Raspberry pi3 / Raspbian ktnet KTCCD323 iWatch 5000萬 網路攝影機 (199元) Install12345678910111213141516# 更新&amp;安裝sudo apt-get updatesudo apt-get upgrade -ysudo apt-get install build-essential libjpeg8-dev imagemagick libv4l-dev cmake -y# Clone Repo 到 /Downloadscd /Downloadsgit clone https://github.com/jacksonliam/mjpg-streamer.gitcd mjpg-streamer/mjpg-streamer-experimental# Makemakesudo make install# 執行/usr/local/bin/mjpg_streamer -i &quot;input_uvc.so -r 1280x720 -d /dev/video0 -f 30 -q 80&quot; -o &quot;output_http.so -p 8080 -w /usr/local/share/mjpg-streamer/www&quot; Servicepi3 IP : 192.168.1.8http-server IP : 192.168.1.161 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width\"&gt; &lt;title&gt;Web Stream&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;XDD&lt;/h1&gt;&lt;img src=\"http://192.168.1.8:8080/?action=stream\" /&gt;&lt;/body&gt;&lt;/html&gt;","link":"/2019/02/02/Raspberry-Pi3-MJPEG-Streamer-Install-Setup/"},{"title":"JavaScript 筆記 - 依序執行function","text":"之前做C3.js圖表平移效果，網頁載入後平移9次，於是寫了9次callback 123456789setTimeout(function(){console.log('shift ', 1)}, 1000);setTimeout(function(){console.log('shift ', 2)}, 2000);setTimeout(function(){console.log('shift ', 3)}, 3000);setTimeout(function(){console.log('shift ', 4)}, 4000);setTimeout(function(){console.log('shift ', 5)}, 5000);setTimeout(function(){console.log('shift ', 6)}, 6000);setTimeout(function(){console.log('shift ', 7)}, 7000);setTimeout(function(){console.log('shift ', 8)}, 8000);setTimeout(function(){console.log('shift ', 9)}, 9000); (｀・ω・´)…….. 呵 宣告function回傳promise12345678const shiftPromise = (round) =&gt; { return new Promise((resolve, reject) =&gt; { setTimeout(() =&gt; { console.log(`shift ${round}`); resolve(round); }, 1000); });} 解法1: resolve()1234567891011Promise.resolve() .then(() =&gt; shiftPromise(1)) .then(() =&gt; shiftPromise(2)) .then(() =&gt; shiftPromise(3)) .then(() =&gt; shiftPromise(4)) .then(() =&gt; shiftPromise(5)) .then(() =&gt; shiftPromise(5)) .then(() =&gt; shiftPromise(6)) .then(() =&gt; shiftPromise(7)) .then(() =&gt; shiftPromise(8)) .then(() =&gt; shiftPromise(9)) 將return的promise一直串下去，但這方法還是很醜，要寫9次 解法2: reduce()展開跟解法1一樣 arr.reduce(callback[accumlator, currentValue, currentIndex, array], initialValue) 123456789[1, 2, 3, 4, 5, 6, 7, 8, 9].reduce((p, current) =&gt; { return p.then(() =&gt; shiftPromise(current))},Promise.resolve()) // 簡寫 去掉return、curly braces[1, 2, 3, 4, 5, 6, 7, 8, 9].reduce( (p, current) =&gt; p.then(() =&gt; shiftPromise(current)), Promise.resolve()) 解法3: async/await這是最簡潔的寫法! 這種情況可以用IIFE去執行 12345(async(dataArray) =&gt; { for(let i = 0; i &lt; dataArray.length; i++) { await shiftPromise(dataArray[i]); }})([1, 2, 3, 4, 5, 6, 7, 8, 9]);","link":"/2019/02/03/js-promise-execute-sequentially/"},{"title":"Support-Vector-Machine演算法 - 紅酒種類預測","text":"scikit-learn 內建dataset : wine dataset 函式庫1234import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns 載入dataset1234from sklearn.datasets import load_winedata = load_wine()data.keys()# print(data.DESCR) 123df = pd.DataFrame(data.data, columns=data.feature_names)df[&apos;target&apos;] = data.targetdf.head() 1sns.pairplot(df, hue='target') 切分訓練、測試資料123456from sklearn.model_selection import train_test_splitX = df.drop('target', axis=1)y = df['target']X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) SVM model123from sklearn.svm import SVCmodel = SVC()model.fit(X_train, y_train) 預測與評估1predictions = model.predict(X_test) 12from sklearn.metrics import classification_report, confusion_matrixprint(confusion_matrix(y_test, predictions)) 123[[ 1 18 0] [ 0 21 0] [ 0 13 1]] 1print(classification_report(y_test, predictions)) 1234567 precision recall f1-score support 0 1.00 0.05 0.10 19 1 0.40 1.00 0.58 21 2 1.00 0.07 0.13 14avg / total 0.77 0.43 0.29 54 GridsearchSVM預設參數可能如上面結果效果不佳，scikit-learn提供方法幫助我們找出適合的C、gamma參數的組合，這種方法稱Gridsearch，可以在==GridSearchCV==調用功能，程式如下 123456from sklearn.grid_search import GridSearchCVparam_grid = {'C': [0.1, 1, 10, 100, 1000, 10000], 'gamma':[1, 0.1, 0.01, 0.001, 0.0001, 0.00001], 'kernel': ['rbf']}grid = GridSearchCV(SVC(), param_grid, verbose=3)grid.fit(X_train, y_train) 到此步驟grid已經套用最好的C、gamma組合 12grid_predictions = grid.predict(X_test)print(confusion_matrix(y_test, grid_predictions)) 123[[17 2 0] [ 0 21 0] [ 0 0 14]] 1print(classification_report(y_test, grid_predictions)) 1234567 precision recall f1-score support 0 1.00 0.89 0.94 19 1 0.91 1.00 0.95 21 2 1.00 1.00 1.00 14avg / total 0.97 0.96 0.96 54 預測效果相比預設參數有大幅改善","link":"/2018/10/12/ML-course-note-SVM/"},{"title":"Minecraft server架設&設定檔紀錄","text":"ShiginimaSE_v3100 (破解版 請自己找)Adderss: yc-minecraft.tk (已關閉) Setting Up a Minecraft Server on Google Compute Engine 官方已提供很詳盡的說明，架完後可用freenom的免費域名(最長1年免費)，就不用記IP。 Info 版本:1.12.2 地圖類型: 大型生態系 pvp: on 作弊: off 人數:20 server.properties1234567891011121314151617181920212223242526272829303132333435363738#Minecraft server properties#Fri Jun 29 04:31:51 UTC 2018max-tick-time=60000generator-settings=force-gamemode=falseallow-nether=truegamemode=0enable-query=falseplayer-idle-timeout=0difficulty=2spawn-monsters=trueop-permission-level=4pvp=truesnooper-enabled=truelevel-type=LARGEBIOMEShardcore=falseenable-command-block=falsemax-players=20network-compression-threshold=256resource-pack-sha1=max-world-size=29999984server-port=25565server-ip=spawn-npcs=trueallow-flight=falselevel-name=worldview-distance=10resource-pack=spawn-animals=truewhite-list=falsegenerate-structures=trueonline-mode=falsemax-build-height=256level-seed=prevent-proxy-connections=falseuse-native-transport=trueenable-rcon=falsemotd=YC Minecraft Server Rule 箱子資源可自取 苦力怕坑要填平(不用填滿) 據點 編號 座標 註 1 127,77,-43 最初重生點附近 2 -4737,65,542 林地宅邸 3 -1477,67,285 沙漠遺跡 4 -1780,69,394 沙漠村莊 5 -1482,67,285 沙漠遺跡 6 - -","link":"/2018/06/21/minecraft-config/"},{"title":"Node.js Debugging 筆記","text":"Command Line node版本一定要是8以上!! 進入debug mode1$node inspect app.js 操作: 單步執行 : 1debug&gt; n #next 進入REPL: 12debug&gt; repl #進入REPL後可以輸入變數名稱查看其值，或執行任何js指令 #用Ctrl+C回到debug mode 全部執行 12debug&gt; c #continue #Ctrl+C離開debug mode 常用技巧: 程式插入中斷點 可以配合nodemon使用 123456789101112//app.jsvar person = { name : 'yc'};person.age = 25;debugger; //中斷點person.name = 'Mike';console.log(person); 執行inspect 1$node inspect app.js 然後continue 1debug&gt; c 如此一來會直接跳到，中斷點 Chrome DevTools1. 進入debug mode1$node --inspect-brk app.js #也可用nodemon 2. 開啟Chrome1URL: chrome://inspect","link":"/2018/05/11/nodejs-debugging/"},{"title":"ModelSim 專案建立&模擬 步驟","text":"Step 1: 用文字編輯器撰寫程式碼隨意路徑隨意(不要有中文) Step 2: 開啟ModelSim，File&gt;New&gt;Project建立Project Project name、Library name隨意命名 按OK後，點選Add Existing File 按Browse選擇剛剛編輯檔案的路徑，按OK完成專案建立 Step 3: 編譯Verilog code 編譯成功或失敗會在Transcript視窗顯示 程式庫(Library)已有對應的檔案 Step 4: 模擬對t(module)按右鍵&gt;Simulate","link":"/2017/09/27/modelSim-create-project-note/"},{"title":"Fackbook Bot Webhook架設筆記","text":"當時參加2018 NASA Hackathon做的Facebook聊天機器人，雖然沒得名，但是學會了FB Bot製作、後端database串接，base64 等等還算是有收穫 呵呵 官方 快速入門教學課程 FB Developers 基本上照著做即可 Note 過程中要記得安裝request套件 在開發者頁面要把VERIFY_TOKEN填入(自己記得就好)，對應到的code的VERIFY_TOKEN 在 Messenger 設定主控台的「權杖產生」區段，點擊「選擇粉絲專頁」下拉式功能表，然後選擇您要這個應用程式訂閱的 Facebook 粉絲專頁。這個粉絲專頁就是當用戶在 Messenger 與其交談時，想要 Webhook 接收事件的粉絲專頁 要訂閱這個粉專!!! 要使這個應用程式上線，需要有隱私權政策網址，可以用產生器產生 要提交pages_messaging功能才能讓開發者以外的人使用訊息 提交審查之前，要先上傳1024x1024 pixel的透明背景圖，以及勾選商業用途 Ref:http://animabeautifullife.blogspot.com/2016/06/facebook-messenger-api.html Ngrok省去webhook在app engine或heroku等等平台部屬的時間，等debug完成再部屬(記得更改webhook網址)，加速開發! Reference: https://blog.techbridge.cc/2018/05/24/ngrok/ Code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294'use strict';const myToken = 'YOUR TOKEN';// Imports dependencies and set up http serverconst express = require('express'), bodyParser = require('body-parser'), app = express().use(bodyParser.json()); // creates express http serverconst request = require('request');const image2base64 = require('image-to-base64'); //upload base64 encode image on firebase// Sets server port and logs message on successapp.listen(process.env.PORT || 3000, () =&gt; console.log('webhook is listening'));app.get('/', (req, res) =&gt; { res.send(`hello yc!!!`);})//1// Creates the endpoint for our webhook app.post('/webhook', (req, res) =&gt; { // Parse the request body from the POST let body = req.body; // Checks this is an event from a page subscription if (body.object === 'page') { // Iterates over each entry - there may be multiple if batched body.entry.forEach(function(entry) { // Gets the message. entry.messaging is an array, but // will only ever contain one message, so we get index 0 let webhook_event = entry.messaging[0]; console.log(webhook_event); // Get the sender PSID let sender_psid = webhook_event.sender.id; console.log('Sender PSID: ' + sender_psid); // Check if the event is a message or postback and // pass the event to the appropriate handler function if (webhook_event.message) { // handleMessage(sender_psid, webhook_event.message); switch(webhook_event.message.text) { case 'hi': webhook_event.message.text = 'Hi :))'; handleMessage(sender_psid, webhook_event.message); break; default: //直接回覆原字串 或傳入attachment handleMessage(sender_psid, webhook_event.message); } } else if (webhook_event.postback) { handlePostback(sender_psid, webhook_event.postback); } }); // Returns a '200 OK' response to all requests res.status(200).send('EVENT_RECEIVED'); } else { // Returns a '404 Not Found' if event is not from a page subscription res.sendStatus(404); } });//2// Adds support for GET requests to our webhookapp.get('/webhook', (req, res) =&gt; { console.log('get webhook'); // Your verify token. Should be a random string. let VERIFY_TOKEN = \"12345\" //對應到FB developer頁面的驗證權杖 // Parse the query params let mode = req.query['hub.mode']; let token = req.query['hub.verify_token']; let challenge = req.query['hub.challenge']; // Checks if a token and mode is in the query string of the request if (mode &amp;&amp; token) { // Checks the mode and token sent is correct if (mode === 'subscribe' &amp;&amp; token === VERIFY_TOKEN) { // Responds with the challenge token from the request console.log('WEBHOOK_VERIFIED'); res.status(200).send(challenge); } else { // Responds with '403 Forbidden' if verify tokens do not match res.sendStatus(403); } }});// Handles messages eventsfunction handleMessage(sender_psid, received_message) { let response; // Checks if the message contains text if (received_message.text) { // Create the payload for a basic text message, which // will be added to the body of our request to the Send API response = { //\"text\": `You sent the message: \"${received_message.text}\". Now send me an attachment! \\nfrom handleMessage` \"attachment\": { \"type\": \"template\", \"payload\": { \"template_type\": \"generic\", \"elements\": [{ \"title\": \"Title test\", \"subtitle\": `subtitle`, \"image_url\": '', \"buttons\": [ { \"type\": \"web_url\", \"url\": 'www.google.com', \"title\":\"Google\" } ], }] } } } } else if (received_message.attachments) { // Get the URL of the message attachment let attachment_url = received_message.attachments[0].payload.url; // console.log(`[XDDDDDDD] ${JSON.stringify(received_message.attachments, undefined, 4)}`); response = { \"attachment\": { \"type\": \"template\", \"payload\": { \"template_type\": \"generic\", \"elements\": [{ \"title\": \"回報系統\", \"subtitle\": `25.024782°N 121.528864°E`, \"image_url\": attachment_url, \"buttons\": [ { \"type\": \"web_url\", \"url\": NASAwebpage, \"title\":\"火災現況\" } ], }] } } } const FirebaseURL = 'https://nasa-hackthon-linebot.firebaseio.com/firespot.json'; image2base64(attachment_url) // you can also to use url .then((response) =&gt; { let MyData = JSON.stringify({ img : response, location: { latitude: 121.528864, longitude: 25.024782 }, status: \"未通報\" }) request({ \"uri\": FirebaseURL, \"method\": \"POST\", \"body\" : MyData }, (err, res, body) =&gt; { if (!err) { //console.log(res); } else { console.log(err); } }); }) .catch((error) =&gt; { console.log(error); //Exepection error.... }); } // Send the response message callSendAPI(sender_psid, response); }// Handles messaging_postbacks eventsfunction handlePostback(sender_psid, received_postback) { let response; // Checks if the message contains text if (received_message.text) { // Creates the payload for a basic text message, which // will be added to the body of our request to the Send API response = { \"text\": `You sent the message: \"${received_message.text}\". Now send me an attachment! \\nfrom handlePostback` } } else if (received_message.attachments) { // Gets the URL of the message attachment let attachment_url = received_message.attachments[0].payload.url; } // Sends the response message callSendAPI(sender_psid, response); }function callSendAPI(sender_psid, response) { // Construct the message body let request_body = { \"recipient\": { \"id\": sender_psid }, \"message\": response } // Send the HTTP request to the Messenger Platform request({ \"uri\": \"https://graph.facebook.com/v2.6/me/messages\", \"qs\": { \"access_token\": myToken }, //PAGE_ACCESS_TOKEN \"method\": \"POST\", \"json\": request_body }, (err, res, body) =&gt; { if (!err) { console.log('message sent!') } else { console.error(\"Unable to send message:\" + err); } }); }// playgroundasync function getLatestInfo() { // Promise let year = new Date().getFullYear() let month = new Date().getMonth() + 1 let date = new Date().getDate() month = (month &lt; 10)? '0'+month : month; date = (date &lt; 10)? '0'+date : date; return new Promise((resolve, reject) =&gt; { request({ \"uri\": `https://launchlibrary.net/1.3/launch/${year}-${month}-${date}`, \"method\": \"GET\", \"json\": true }, (err, res, body) =&gt; { if (!err) { // console.log(`[QUERY] ${year}-${month}-${date}`) // console.log(body.launches[0]) let latest = body.launches[0]; resolve({ date: latest.windowstart, name: latest.name, lat: latest.location.pads[0].latitude, lng: latest.location.pads[0].longitude, gmap: latest.location.pads[0].mapURL }) // console.log(data); } else { reject(\"Unable to send message:\" + err); } }); })}/*Usage:getLatestInfo().then((res) =&gt; { // handle }).catch((err) =&gt; { // error })*/var fbIDtoTimestamp = (id) =&gt; { var PUSH_CHARS = \"-0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz\"; id = id.substring(0,8); var timestamp = 0; for (var i=0; i &lt; id.length; i++) { var c = id.charAt(i); timestamp = timestamp * 64 + PUSH_CHARS.indexOf(c); } return timestamp;}","link":"/2018/10/22/bulid-a-facebook-bot/"},{"title":"Tensorflow Notebook 2 - Linear Regression","text":"Setup Download Python 3.7 version 勾選新增環境變數 (沒勾就要手動加，不然就開Anaconda Prompt下指令也可) cd到教學資料夾，有tfdl_enc.yml檔案 執行conda env create -f tfdl_env.yml 執行activate tfdeeplearning 執行jupyter notebook Basic Training Flow基本流程 建立Graph 初始化Session 丟入data並取得輸出 Linear Regression 1234import numpy as npimport tensorflow as tfimport matplotlib.pyplot as plt%matplotlib inline 12x_data = np.linspace(0,10,10) + np.random.uniform(-1.5,1.5,10) # 加入雜訊y_label = np.linspace(0,10,10) + np.random.uniform(-1.5,1.5,10) 模擬實際資料分布1plt.plot(x_data,y_label,'o') 給定隨機斜率與截距 12m = tf.Variable(0.5) # 隨便給的variable ~b = tf.Variable(0.5) Cost Function如同吳恩達在Linear Regression所講的cost function$$J(\\theta_0, \\theta_1) = \\frac{ 1 }{ 2m } \\displaystyle \\sum_{ i = 1 }^{ m } (\\hat{ y }^{(i)} - y^{(i)})^2 = \\frac{ 1 }{ 2m } \\displaystyle \\sum_{ i = 1 }^{ m } (h_\\theta(x^{(i)}) - y^{(i)})^2 $$ $\\theta_0$是截距b，$\\theta_1$是斜率m $\\hat{ y }$或是$h_\\theta(x)$就是預測的$y$值所以預測與現實的差距$(\\hat{ y } - y)$就是誤差，而平方目的是punish，讓誤差大時累加error更大，並在誤差減少時快速遞減 1234567error = 0for x,y in zip(x_data,y_label): y_hat = m*x + b # hypothesis error += (y_hat - y)**2 梯度下降法最小化Cost Function12optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001) # learning_rate通常就用0.001train = optimizer.minimize(error) train存的是optimizer傳入cost function(error)回傳的operation，所以之後要丟入session.run() 初始化 Variables1init = tf.global_variables_initializer() Session12345678910111213with tf.Session() as sess: sess.run(init) epochs = 100 # 1個epoch表示跑過train set中所有樣本一次 for i in range(epochs): sess.run(train) final_slope ,final_intercept = sess.run([m,b]) print('%d次epochs的線性回歸函數: y = %.2fx + %.2f ' % (epochs, final_slope, final_intercept))# 100次epochs的線性回歸函數: y = 0.91x + 0.42 Result12345x_test = np.linspace(-1,11,10)y_pred_plot = final_slope * x_test + final_interceptplt.plot(x_test,y_pred_plot,'r')plt.plot(x_data,y_label,'o') 實驗1123456import numpy as npimport pandas as pdimport tensorflow as tfimport matplotlib.pyplot as plt%matplotlib inline 123x_data = np.linspace(0.0, 10.0, 1000000) noise = np.random.randn(len(x_data)) $y = wx + b + noise$ 1y_true = (0.5 * x_data) + 5 + noise 123456789x_df = pd.DataFrame(data=x_data, columns=['X'])y_df = pd.DataFrame(data=y_true, columns=['Y'])my_data = pd.concat([x_df, y_df], axis=1)my_data.head()my_data.sample(n=250).plot(kind='scatter', x='X', y='Y') 12m = tf.Variable(0.5) # random numbersb = tf.Variable(1.0) 12x_ph = tf.placeholder(tf.float32, [batch_size])y_ph = tf.placeholder(tf.float32, [batch_size]) 預測函數:$\\hat{ y } = m * x + b$ 1y_hat = m * x_ph + b 使用tensorflow的API來描述cost function1error = tf.reduce_sum(tf.square(y_hat - y_ph)) 梯度下降法optimizer 12optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)train = optimizer.minimize(error) 初始化graph中的global variables，回傳值是operation 1init = tf.global_variables_initializer() Session: 定義bach size 1234567891011121314batch_size = 8with tf.Session() as sess: sess.run(init) # 每一批丟入8個實際(x, y)值，optimizer會最佳化回歸直線，共做1000次 batches = 1000 for i in range(batches): rand_idx = np.random.randint(len(x_data), size = batch_size) # 回傳隨機8個index值的陣列 feed = {x_ph: x_data[rand_idx], y_ph: y_true[rand_idx]} sess.run(train, feed_dict=feed) final_slope ,final_intercept = sess.run([m,b]) # 記得m,b是上面宣告的tf.Variable() 123y_hat = final_slope * x_data + final_interceptmy_data.sample(250).plot(kind='scatter', x='X', y='Y')plt.plot(x_data, y_hat, 'r') 最後training的結果(圖中紅線) 實驗2 - 使用estimator API123456import numpy as npimport pandas as pdimport tensorflow as tfimport matplotlib.pyplot as plt%matplotlib inline 產生樣本 1x_data = np.linspace(0.0, 10.0, 1000000) 加入雜訊 1noise = np.random.uniform(-2, 2, len(x_data)) $y = wx + b + noise$ 1y_true = (0.8 * x_data) + 3 + noise 畫出模擬的資料分布 12myDataset = pd.concat([pd.DataFrame(data=x_data,columns=['X Data']),pd.DataFrame(data=y_true,columns=['Y'])],axis=1)myDataset.sample(250).plot(kind='scatter',x='X Data',y='Y') 使用estimator API來做訓練123# 首先要設定feature類型、倒入哪種模型，可以想成格式設定feat_cols = [tf.feature_column.numeric_column('x',shape=[1])] estimator = tf.estimator.LinearRegressor(feature_columns=feat_cols) 切分訓練、測試資料123from sklearn.model_selection import train_test_splitx_train, x_eval, y_train, y_eval = train_test_split(x_data,y_true,test_size=0.3, random_state = 101) 丟入樣本，類似之前的feed_dict動作1234input_func = tf.estimator.inputs.numpy_input_fn({'x':x_train},y_train,batch_size=4,num_epochs=None,shuffle=True)train_input_func = tf.estimator.inputs.numpy_input_fn({'x':x_train},y_train,batch_size=4,num_epochs=1000,shuffle=False) # 接受參數，輸出數據訓練數據eval_input_func = tf.estimator.inputs.numpy_input_fn({'x':x_eval},y_eval,batch_size=4,num_epochs=1000,shuffle=False) # 接受參數，並輸出驗證數據和測試數據 Trains a model given training data input_fn 1estimator.train(input_fn=input_func,steps=1000) Evaluation12train_metrics = estimator.evaluate(input_fn=train_input_func,steps=1000)eval_metrics = estimator.evaluate(input_fn=eval_input_func,steps=1000) 1234print(\"train metrics: {}\".format(train_metrics))print(\"eval metrics: {}\".format(eval_metrics))#train metrics: {'average_loss': 1.393801, 'global_step': 1000, 'loss': 5.5752039}#eval metrics: {'average_loss': 1.3947791, 'global_step': 1000, 'loss': 5.5791163} Predictions12345678910111213141516# 假設我們得到一組新的樣本 np.linspace(0, 10, 10)input_fn_predict = tf.estimator.inputs.numpy_input_fn({'x':np.linspace(0,10,10)},shuffle=False)list(estimator.predict(input_fn=input_fn_predict))# fetch resultspredictions = []for x in estimator.predict(input_fn=input_fn_predict): predictions.append(x['predictions'])predictionsmyPredict = pd.concat([pd.DataFrame(data=np.linspace(0,10,10),columns=['New X']), pd.DataFrame(data=predictions,columns=['predictions'])],axis=1)print('my prediction line: y = %.2fx + %.2f' % ((myPredict.iloc[1]['predictions'] - myPredict.iloc[0]['predictions'])/((myPredict.iloc[1]['New X'] - myPredict.iloc[0]['New X'])), myPredict.iloc[0]['predictions'])) 預測函數: $y = 0.81x + 2.75$ 12myDataset.sample(n=250).plot(kind='scatter',x='X Data',y='Y')plt.plot(np.linspace(0,10,10), predictions, 'r')","link":"/2019/02/04/Tensorflow-notebook-2/"},{"title":"Arduino LoRa Shield通訊實驗與NodeRED","text":"HW41.分別針對不同Spreading Factor情況下LoRa在移動下（可騎承腳踏車、搭公車、搭捷運、搭貓纜）所傳送資料的狀況做記錄（例：Packet Error Rate） 頻率設定位置: setFrequency() in the file: Sketchbook\\libraries\\RadioHead\\RH_RF95.cpp;Line 114: setFrequency(918.7); 1234567typedef enum { Bw125Cr45Sf128 = 0, ///&lt; Bw = 125 kHz, Cr = 4/5, Sf = 128chips/symbol, CRC on. Default medium range Bw500Cr45Sf128, ///&lt; Bw = 500 kHz, Cr = 4/5, Sf = 128chips/symbol, CRC on. Fast+short range Bw31_25Cr48Sf512, ///&lt; Bw = 31.25 kHz, Cr = 4/8, Sf = 512chips/symbol, CRC on. Slow+long range Bw125Cr48Sf4096, ///&lt; Bw = 125 kHz, Cr = 4/8, Sf = 4096chips/symbol, CRC on. Slow+long range } ModemConfigChoice; source codemyLoraServerSFtest-FieldTrailTx.ino123456789101112131415161718192021222324252627282930313233343536#include &lt;SPI.h&gt;#include &lt;RH_RF95.h&gt;RH_RF95 rf95;long counter=0;void setup() { Serial.begin(9600); delay(1000); while (!Serial) ; // Wait for serial port to be available if (!rf95.init()) Serial.println(\"init failed\"); Serial.println(); Serial.println(\"Lora init passed\"); rf95.setModemConfig(RH_RF95::Bw125Cr48Sf4096); // failed // rf95.setModemConfig(RH_RF95::Bw125Cr45Sf128); // test OK (Default medium range) // rf95.setModemConfig(RH_RF95::Bw500Cr45Sf128); // test OK (Fast + short range) // rf95.setModemConfig(RH_RF95::Bw31_25Cr48Sf512); // test OK (Slow + long range適用長距離) rf95.setTxPower(23, false);}void loop() { counter++; String numString = \"Server: \"+ String(counter, DEC)+\" \"; //加空白以去除亂碼 uint8_t data[14] = \"\"; for (int i=0; i&lt;14; i++) { data[i] =numString[i]; // } rf95.send(data, sizeof(data)); rf95.waitPacketSent(); Serial.println(numString); delay(1000);} myLoraClientSFtest-FieldTrailRx.ino1234567891011121314151617181920212223242526272829303132333435363738#include &lt;SPI.h&gt;#include &lt;RH_RF95.h&gt;#include &lt;Wire.h&gt;RH_RF95 rf95;int counter=0;void setup() { Serial.begin(9600); while (!Serial) ; if (!rf95.init()) Serial.println(\"init failed\"); Serial.println(\"init passed\"); rf95.setModemConfig(RH_RF95::Bw125Cr48Sf4096); // rf95.setModemConfig(RH_RF95::Bw125Cr45Sf128); // rf95.setModemConfig(RH_RF95::Bw500Cr45Sf128); // rf95.setModemConfig(RH_RF95::Bw31_25Cr48Sf512); rf95.setTxPower(23, false);}void loop(){ if (rf95.available()) { uint8_t buf[14]; uint8_t len = sizeof(buf); if (rf95.recv(buf, &amp;len)) { counter++; Serial.print(\"Rx: \"); Serial.println((char*)buf); Serial.print(\"RSSI: \"); Serial.println(rf95.lastRssi(), DEC); } }} 2.不同長度的展頻symbol數，會造成不同的傳送完成時間，請量測不同Spreading Factor傳送完成所需時間紀錄。source code1234567891011121314151617181920212223242526272829303132333435363738#include &lt;SPI.h&gt;#include &lt;RH_RF95.h&gt;RH_RF95 rf95;long counter=0;void setup() { Serial.begin(9600); delay(1000); while (!Serial) ; // Wait for serial port to be available if (!rf95.init()) Serial.println(\"init failed\"); Serial.println(); Serial.println(\"Lora init passed\"); rf95.setModemConfig(RH_RF95::Bw125Cr48Sf4096); // failed // rf95.setModemConfig(RH_RF95::Bw125Cr45Sf128); // test OK (Default medium range) // rf95.setModemConfig(RH_RF95::Bw500Cr45Sf128); // test OK (Fast + short range) // rf95.setModemConfig(RH_RF95::Bw31_25Cr48Sf512); // test OK (Slow + long range適用長距離) rf95.setTxPower(23, false); //傳送1100筆資料 for(int j = 0; j &lt;= 1100; j ++) { counter++; String numString = \"Server: \"+ String(counter, DEC)+\" \"; //加空白以去除亂碼 uint8_t data[14] = \"\"; for (int i=0; i&lt;14; i++) { data[i] =numString[i]; // } rf95.send(data, sizeof(data)); rf95.waitPacketSent(); Serial.println(numString); }}void loop() {} RPi Serial monitor with timestamp1234567891011import timefrom pytz import timezone # pip install pytzfrom datetime import datetimeimport serialser = serial.Serial('/dev/ttyACM0', 9600)while True: u = datetime.now(timezone('Asia/Taipei')) print(u.strftime('%H:%M:%S.%f')[:-3] + \" &gt;&gt; \" + ser.readline()) 使用樹莓派讀取serial好處是可以有準確的unix timestamp，只要有行動電源與手機(AP)就能到戶外進行實測，並直接將結果存成log.txt檔案放在RPi 參考 timestamp 樹莓派 https://stackoverflow.com/questions/10997577/python-timezone-conversion pytz-time-zones.py : https://gist.github.com/heyalexej/8bf688fd67d7199be4a1682b3eec7568 RadioHead API : https://www.airspayce.com/mikem/arduino/RadioHead/classRHGenericDriver.html#a0feda1f5522522dc50e0c26dcdef71dd HW5PM2.5 測試程式12345678910111213141516171819202122int dustPin=0; //A0float dustVal=0; int delayTime=280;int delayTime2=40;float offTime=9680;void setup(){ Serial.begin(9600); pinMode(dustPin, INPUT);} void loop(){ delayMicroseconds(delayTime); dustVal = analogRead(dustPin); delayMicroseconds(delayTime2); delayMicroseconds(offTime); delay(1000); if (dustVal&gt;36.455) Serial.println((float(dustVal/1024)-0.0356)*120000*0.035);} LoRa Client(TX) 傳輸PM2.5資料123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;SPI.h&gt;#include &lt;RH_RF95.h&gt;RH_RF95 rf95;int count = 0;// PM2.5int dustPin=0; //A0float dustVal=0;float AirQ;String AirQT;void setup() { Serial.begin(9600); pinMode(dustPin, INPUT); delay(1000); while (!Serial) ; // Wait for serial port to be available if (!rf95.init()) Serial.println(&quot;init failed&quot;); Serial.println(); Serial.println(&quot;Lora init passed Bw31_25Cr48Sf512&quot; ); rf95.setModemConfig(RH_RF95::Bw125Cr45Sf128); // test OK (Default medium range) rf95.setTxPower(23, false); }void loop() { count++; dustVal = analogRead(dustPin); PM25Info(&amp;AirQ, &amp;AirQT); String numString = (String)count + &quot;,&quot; + (String)AirQ + &quot;,&quot; + AirQT + &quot; &quot;; //加空白以去除亂碼 uint8_t data[14] = &quot;&quot;; for (int i=0; i&lt;14; i++) { data[i] =numString[i]; // } rf95.send(data, sizeof(data)); rf95.waitPacketSent(); Serial.println(numString); delay(1000);}void PM25Info(float* aq, String* aqt) { if (dustVal&gt;36.455) *aq = float((dustVal/1024)-0.0356)*120000*0.035; if (*aq &lt; 300) { *aqt=&quot; Good &quot;;} if (*aq &gt;= 300 &amp;&amp; *aq &lt; 1050) { *aqt=&quot; Moderate&quot;;} if (*aq &gt;= 1050 &amp;&amp; *aq &lt; 3000) { *aqt=&quot; Unhealthy&quot;;} if (*aq &gt; 3000) { *aqt=&quot; Hazardous&quot;;}} LoRa Server(RX) 接收PM2.5資料1234567891011121314151617181920212223242526272829#include &lt;SPI.h&gt;#include &lt;RH_RF95.h&gt;RH_RF95 rf95;void setup() { Serial.begin(9600); while (!Serial) ; if (!rf95.init()) Serial.println(&quot;init failed&quot;); Serial.println(&quot;init passed Bw125Cr45Sf128&quot;); rf95.setModemConfig(RH_RF95::Bw125Cr45Sf128); rf95.setTxPower(23, false);}void loop(){ if (rf95.available()) { uint8_t buf[14]; uint8_t len = sizeof(buf); if (rf95.recv(buf, &amp;len)) { Serial.print(&quot;Rx: &quot;); Serial.println((char*)buf); Serial.print(&quot;RSSI: &quot;); Serial.println(rf95.lastRssi(), DEC); } }} NodeRED 環境資料整合GatewayMQTT broker : rpi publisher: rpi subscriber: PC 由serial顯示LoRa Server(RX) 接收的PM2.5數值 解析payload並mqtt publishnodes: serial node:data switch function node(parse pm2.5): function node(parse rssi): mqtt node (pm2.5): mqtt node (RSSI): 接收topic: home/LoRa的subscriber程式1234567891011121314151617181920212223242526var mqtt = require('mqtt')var client = mqtt.connect('mqtt://192.168.0.105') //broker ip (rpi)var client2 = mqtt.connect('mqtt://192.168.0.105') client.on('connect', function () { console.log('mqtt broker connected!'); client.subscribe('home/LoRa/pm2.5');}) client.on('message', function (topic, msg) { let info = JSON.parse(msg) console.log('\\n收到pm2.5 topic: ' + topic); console.log(`時間: ${info.time} \\npm2.5: ${info.pm25} \\nstatus: ${info.status}`);})// for rssi client2.on('connect', function () { console.log('mqtt broker connected!'); client2.subscribe('home/LoRa/rssi'); }) client2.on('message', function (topic, msg) { let info = JSON.parse(msg) console.log('\\ntopic: home/LoRa/rssi') console.log(\"RSSI: \" + info.rssi)}) GUI 需先安裝node-red-dashboardManage palette &gt; install &gt; 輸入node-red-dashboard nodes: get pm2.5 value function Publish溫溼度(DHT22)資料1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;SPI.h&gt;#include &lt;Ethernet.h&gt;#include &lt;PubSubClient.h&gt;#include \"DHT.h\"#define DHTPIN 2 // what digital pin we're connected to#define DHTTYPE DHT22 // DHT 22 (AM2302), AM2321DHT dht(DHTPIN, DHTTYPE);// set MAC addressstatic byte mac[] = { 0xF0, 0x7B, 0xCB, 0x4B, 0x7C, 0x9F };// set server and UNO IP adderss IPAddress ip(192, 168, 0, 108);IPAddress server(192, 168, 0, 105); // broker ip// Client IDconst char clientID[] = \"uno\";// topicconst char topic[] = \"home/dht22\";String msgStr = \"\";char json[25];EthernetClient ethClient; // Ethernet objectPubSubClient client(ethClient); // MQTT object basedon Ethernetvoid setup(){ Serial.begin(9600); dht.begin(); client.setServer(server, 1883); //broker and port Ethernet.begin(mac, ip); // initialize Ethernet delay(1500);}void loop(){ if (!client.connected()) { reconnect(); } client.loop(); // update MQTT client float h = dht.readHumidity() float t = dht.readTemperature();// Read temperature as Celsius (the default) // JSON format string msgStr = msgStr + \"{\\\"temp\\\":\" + (String)t + \",\\\"humid\\\":\" + (String)h + \"}\"; // turn to char array, store in json msgStr.toCharArray(json, 25); client.publish(topic, json); msgStr = \"\"; // clean delay(5000);} 加溫溼度的nodes","link":"/2018/11/15/iot-course2018-HW4-HW5/"},{"title":"透過AWS IoT core儲存sensor資料至DynamoDB","text":"這是物聯網導論HW6，使用AWS IoT core與物聯網裝置經由MQTT通訊，儲存到DynamoDB，再由後端使用matplotlib繪製感測器資訊。 入門照著步驟做，先安裝pip install AWSIoTPythonSDK AWS IoT subscribe PM2.5 MQTT修改政策IOT CORE -&gt; 安全無虞 -&gt; 政策 -&gt; {政策的名稱} -&gt; 編輯政策文件 發布、訂閱程式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# basicPubSub_PM2.5.pyfrom AWSIoTPythonSDK.MQTTLib import AWSIoTMQTTClientfrom serial import Serialimport loggingimport timefrom pytz import timezone # pip install pytzfrom datetime import datetimeimport argparseimport jsonAllowedActions = ['both', 'publish', 'subscribe']# Custom MQTT message callbackdef customCallback(client, userdata, message): print(\"Received a new message: \") print(message.payload) print(\"from topic: \") print(message.topic) print(\"--------------\\n\\n\")def publish(PM25): if mode == 'both' or mode == 'publish': u = datetime.now(timezone('Asia/Taipei')) message = {} message['type'] = 'PM2.5' message['value'] = PM25 message['time'] =u.strftime('%H:%M:%S') messageJson = json.dumps(message) myAWSIoTMQTTClient.publish(topic, messageJson, 1) if mode == 'publish': print('Published topic %s: %s\\n' % (topic, messageJson))host = \"a1edmqmbezxdwv-ats.iot.us-east-1.amazonaws.com\"rootCAPath = \"./root-CA.crt\"certificatePath = \"./RPi.cert.pem\"privateKeyPath = \"./RPi.private.key\"useWebsocket = FalseclientId = \"vensen\"topic = \"RPi/PM25\"mode = \"publish\"# Port defaultsif useWebsocket: # When no port override for WebSocket, default to 443 port = 443if not useWebsocket: # When no port override for non-WebSocket, default to 8883 port = 8883# Configure logginglogger = logging.getLogger(\"AWSIoTPythonSDK.core\")logger.setLevel(logging.DEBUG)streamHandler = logging.StreamHandler()formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')streamHandler.setFormatter(formatter)logger.addHandler(streamHandler)# Init AWSIoTMQTTClientmyAWSIoTMQTTClient = Noneif useWebsocket: myAWSIoTMQTTClient = AWSIoTMQTTClient(clientId, useWebsocket=True) myAWSIoTMQTTClient.configureEndpoint(host, port) myAWSIoTMQTTClient.configureCredentials(rootCAPath)else: myAWSIoTMQTTClient = AWSIoTMQTTClient(clientId) myAWSIoTMQTTClient.configureEndpoint(host, port) myAWSIoTMQTTClient.configureCredentials(rootCAPath, privateKeyPath, certificatePath)# AWSIoTMQTTClient connection configurationmyAWSIoTMQTTClient.configureAutoReconnectBackoffTime(1, 32, 20)myAWSIoTMQTTClient.configureOfflinePublishQueueing(-1) # Infinite offline Publish queueingmyAWSIoTMQTTClient.configureDrainingFrequency(2) # Draining: 2 HzmyAWSIoTMQTTClient.configureConnectDisconnectTimeout(10) # 10 secmyAWSIoTMQTTClient.configureMQTTOperationTimeout(5) # 5 sec# Connect and subscribe to AWS IoTmyAWSIoTMQTTClient.connect()if mode == 'both' or mode == 'subscribe': myAWSIoTMQTTClient.subscribe(topic, 1, customCallback)time.sleep(2)# Publish to the same topic in a loop foreverser = Serial('/dev/ttyACM0', 9600)while True: str = ser.readline().decode('utf8')[:-2] end = str.find('\\r') PM25 = str[:end] publish(PM25) time.sleep(2) 放在與start.sh同一層路徑下 Host在start.sh內(Line 21)123456789101112131415161718192021# stop script on errorset -e# Check to see if root CA file exists, download if notif [ ! -f ./root-CA.crt ]; then printf \"\\nDownloading AWS IoT Root CA certificate from AWS...\\n\" curl https://www.amazontrust.com/repository/AmazonRootCA1.pem &gt; root-CA.crtfi# install AWS Device SDK for Python if not already installedif [ ! -d ./aws-iot-device-sdk-python ]; then printf \"\\nInstalling AWS SDK...\\n\" git clone https://github.com/aws/aws-iot-device-sdk-python.git pushd aws-iot-device-sdk-python python setup.py install popdfi# run pub/sub sample app using certificates downloaded in packageprintf \"\\nRunning pub/sub sample application...\\n\"python aws-iot-device-sdk-python/samples/basicPubSub/basicPubSub.py -e a3l21ss4gwlsqf-ats.iot.us-east-1.amazonaws.com -r root-CA.crt -c pi3.cert.pem -k pi3.private.key 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687from AWSIoTPythonSDK.MQTTLib import AWSIoTMQTTClientfrom serial import Serialimport loggingimport timefrom pytz import timezone # pip install pytzfrom datetime import datetimeimport argparseimport jsonimport randomAllowedActions = ['both', 'publish', 'subscribe']# Custom MQTT message callbackdef customCallback(client, userdata, message): print(\"Received a new message: \") print(message.payload) print(\"from topic: \") print(message.topic) print(\"--------------\\n\\n\")def publish(PM25): if mode == 'both' or mode == 'publish': u = datetime.now(timezone('Asia/Taipei')) message = {} message['time'] = u.strftime('%H:%M:%S') message['value'] = PM25 messageJson = json.dumps(message) myAWSIoTMQTTClient.publish(topic, messageJson, 1) if mode == 'publish': print('Published topic %s: %s\\n' % (topic, messageJson))host = \"a3l21ss4gwlsqf-ats.iot.us-east-1.amazonaws.com\"rootCAPath = \"./root-CA.crt\"certificatePath = \"./RPi.cert.pem\"privateKeyPath = \"./RPi.private.key\"useWebsocket = FalseclientId = \"myRPi\"topic = \"RPi/PM25\"mode = \"publish\"# Port defaultsif useWebsocket: # When no port override for WebSocket, default to 443 port = 443if not useWebsocket: # When no port override for non-WebSocket, default to 8883 port = 8883# Configure logginglogger = logging.getLogger(\"AWSIoTPythonSDK.core\")logger.setLevel(logging.DEBUG)streamHandler = logging.StreamHandler()formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')streamHandler.setFormatter(formatter)logger.addHandler(streamHandler)# Init AWSIoTMQTTClientmyAWSIoTMQTTClient = Noneif useWebsocket: myAWSIoTMQTTClient = AWSIoTMQTTClient(clientId, useWebsocket=True) myAWSIoTMQTTClient.configureEndpoint(host, port) myAWSIoTMQTTClient.configureCredentials(rootCAPath)else: myAWSIoTMQTTClient = AWSIoTMQTTClient(clientId) myAWSIoTMQTTClient.configureEndpoint(host, port) myAWSIoTMQTTClient.configureCredentials(rootCAPath, privateKeyPath, certificatePath)# AWSIoTMQTTClient connection configurationmyAWSIoTMQTTClient.configureAutoReconnectBackoffTime(1, 32, 20)myAWSIoTMQTTClient.configureOfflinePublishQueueing(-1) # Infinite offline Publish queueingmyAWSIoTMQTTClient.configureDrainingFrequency(2) # Draining: 2 HzmyAWSIoTMQTTClient.configureConnectDisconnectTimeout(10) # 10 secmyAWSIoTMQTTClient.configureMQTTOperationTimeout(5) # 5 sec# Connect and subscribe to AWS IoTmyAWSIoTMQTTClient.connect()if mode == 'both' or mode == 'subscribe': myAWSIoTMQTTClient.subscribe(topic, 1, customCallback)time.sleep(2)# Publish to the same topic in a loop foreverser = Serial('/dev/ttyACM0', 9600)while True: str = ser.readline().decode('utf8')[:-2] end = str.find('\\r') PM25 = str[:end] publish(PM25) 在VM執行 fetch DynamoDB基本連線1234567891011121314151617import boto3from boto3.dynamodb.conditions import Keydynamodb = boto3.resource('dynamodb', region_name='us-east-1', aws_session_token='FQoGZXIvYXdzEIz//////////wEaDEmGC67DXliftdi15yLjAlNe8Meiqw9LjtH0bS2WCsEwKrtZdGVj4lEXx7YlsON2f3I6vQ4FRqen8d+hT/zUpVzDb6JVMblgslHXSUlaQWi8qovFVQtbw/AvtdlSiZlH3nENUGpev5rlVe30dl5DAiFj8HIZV47+zIcsfZXRqkL8faMpnssTCNqVRwcF4iX5U9o6vjHvMVYelW+EnUVU9/Le2bTe5QDcUsvhAOvzzZ4ZlE/SFEYfFQ4/W3cbr83NLEvU4Bqj8bqKu8L2Vmotbcong/3SFe/QgJH6KsN5Q28HDnjtnvGxFf3z2v/lGOV9e+qVWNFht5/nh6dk+JXni59hXibZa7LIZxHKAOVqs+Vk8es9JVUqm4EYH16e5fHqHLG9gSPJG64RVBa8Jth1hCjWryAlitWFdk1FTiuHdsLYIBRuPz54sdFFJFGoudEnC7/C/8eZPO+H0JseakCFZNMv9zm92cXYoJ9bS/eIu10s0Ykoi8iJ4AU=', aws_access_key_id = 'ASIA27E3PJ447BBJPJNA', aws_secret_access_key = 'taQBfl4HxZF6xyzQvflv2BT04p5sy6dMwKyeo1CE' )response = table.get_item( Key = {'time': '14:00:01'}) # 取出特定時間資料try: # print(response) item = response['Item'] message = item['message'] print(message['value'])except KeyError: print('') source code取出DB資料，存成log.txt123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import boto3from boto3.dynamodb.conditions import Keydynamodb = boto3.resource('dynamodb', region_name='us-east-1', aws_session_token='FQoGZXIvYXdzEIz//////////wEaDEmGC67DXliftdi15yLjAlNe8Meiqw9LjtH0bS2WCsEwKrtZdGVj4lEXx7YlsON2f3I6vQ4FRqen8d+hT/zUpVzDb6JVMblgslHXSUlaQWi8qovFVQtbw/AvtdlSiZlH3nENUGpev5rlVe30dl5DAiFj8HIZV47+zIcsfZXRqkL8faMpnssTCNqVRwcF4iX5U9o6vjHvMVYelW+EnUVU9/Le2bTe5QDcUsvhAOvzzZ4ZlE/SFEYfFQ4/W3cbr83NLEvU4Bqj8bqKu8L2Vmotbcong/3SFe/QgJH6KsN5Q28HDnjtnvGxFf3z2v/lGOV9e+qVWNFht5/nh6dk+JXni59hXibZa7LIZxHKAOVqs+Vk8es9JVUqm4EYH16e5fHqHLG9gSPJG64RVBa8Jth1hCjWryAlitWFdk1FTiuHdsLYIBRuPz54sdFFJFGoudEnC7/C/8eZPO+H0JseakCFZNMv9zm92cXYoJ9bS/eIu10s0Ykoi8iJ4AU=', aws_access_key_id = 'ASIA27E3PJ447BBJPJNA', aws_secret_access_key = 'taQBfl4HxZF6xyzQvflv2BT04p5sy6dMwKyeo1CE' )## utiliy function ####################################def toTimeStamp(u): HH = str(u/3600) if u/3600 &gt;= 10 else '0' + str(u/3600); u %= 3600; MM = str(u/60) if u/60 &gt;= 10 else '0' + str(u/60); SS = str(u%60) if u%60 &gt;= 10 else '0' + str(u%60); return HH + ':' + MM + ':' + SS;def toUnixTime(time): t = time.split(':'); i = 3600; sum = 0; for a in t: sum += int(a) * i; i /= 60; return sum;def getMoveAvgBy30secRange(u1): # trace back 30 sec and calculate move average res = []; u2 = u1 - 30; for i in range(u1, u2, -1): print('&gt;&gt;' + toTimeStamp(i)) response = table.get_item(Key={'time': toTimeStamp(i)}) try: item = response['Item'] message = item['message'] res.append(message['value']); except KeyError: print('') sum = 0; for x in res: sum += x; moveAvg = sum / len(res); return {toTimeStamp(u1) : moveAvg};#####################################################table = dynamodb.Table('PM25sensor')# settingstart_hour = '15:00:00';total = 1;list_30sec_avg = [];list_2min__avg = [];list_10min_avg = [];f = open(\"log.txt\", \"a\")# list of 30 sec move avgt1 = toUnixTime(start_hour);end = toTimeStamp(t1 - total * 3600);t2 = toUnixTime(end);print(start_hour + 'trace back to '+ end)while(t1 != t2): # list_30sec_avg.append(getMoveAvgBy30secRange(t1)); print(list_30sec_avg); t1 -= 30;print(list_30sec_avg);f.write(str(list_30sec_avg)); 產生移動平均chart123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltfrom scipy import statsf = open('log.txt', 'r')data = f.read().split('{');data = data[1:];# print(data[1:])moveAvg_30sec = [];timeList = [];pm25List = [];for e in data: # print(e) time = e[1:9]; timeList.append(time) # print(time); s = e.split(\"': Decimal('\"); pm25 = s[1].split(\"'\")[0] pm25_float = float(pm25) pm25List.append(pm25_float) moveAvg_30sec.append({time: pm25});# visualizationdf = pd.DataFrame({'time':timeList, 'pm2.5': pm25List})# df['pm2.5'] = df['pm2.5'].astype('float64') # convert pm2.5 string to floatdf['index_col'] = df.index;print(df.head(5));fig, ax = plt.subplots(1,1)ax.set_xticklabels(df['time'], rotation=15, fontsize=18)sns.lineplot(x=\"index_col\", y=\"pm2.5\", data=df)plt.title(\"30 sec move average\")# plt.show()##### 2 min move avg# clear listtimeList = [];pm25List = [];moveAvg_2min = []for i in range(0, len(moveAvg_30sec) - len(moveAvg_30sec)%4, 4): time = moveAvg_30sec[i].keys()[0]; avg2 = (float(moveAvg_30sec[i].values()[0]) + float(moveAvg_30sec[i + 1].values()[0]) + float(moveAvg_30sec[i + 2].values()[0]) + float(moveAvg_30sec[i + 3].values()[0])) / 4; moveAvg_2min.append({time: avg2});# print (moveAvg_2min)for e in moveAvg_2min: # print(e) time = e.keys()[0]; timeList.append(time) pm25 = e.values()[0]; pm25_float = float(pm25) pm25List.append(pm25_float)# visualizationdf = pd.DataFrame({'time':timeList, 'pm2.5': pm25List})# df['pm2.5'] = df['pm2.5'].astype('float64') # convert pm2.5 string to floatdf['index_col'] = df.index;print(df.head(5));fig, ax = plt.subplots(1,1)ax.set_xticklabels(df['time'], rotation=15, fontsize=18)sns.lineplot(x=\"index_col\", y=\"pm2.5\", data=df)plt.title(\"2 min move average\")plt.show()##### 10 min move avg# clear listtimeList = [];pm25List = [];moveAvg_10min = []for i in range(0, len(moveAvg_30sec) - len(moveAvg_30sec)%20, 20): time = moveAvg_30sec[i].keys()[0]; avg10 = (float(moveAvg_30sec[i].values()[0]) + float(moveAvg_30sec[i + 1].values()[0]) + float(moveAvg_30sec[i + 2].values()[0]) + float(moveAvg_30sec[i + 3].values()[0]) + float(moveAvg_30sec[i + 4].values()[0]) + float(moveAvg_30sec[i + 5].values()[0]) + float(moveAvg_30sec[i + 6].values()[0]) + float(moveAvg_30sec[i + 7].values()[0]) + float(moveAvg_30sec[i + 8].values()[0]) + float(moveAvg_30sec[i + 9].values()[0]) + float(moveAvg_30sec[i + 10].values()[0]) + float(moveAvg_30sec[i + 11].values()[0]) + float(moveAvg_30sec[i + 12].values()[0]) + float(moveAvg_30sec[i + 13].values()[0]) + float(moveAvg_30sec[i + 14].values()[0]) + float(moveAvg_30sec[i + 15].values()[0]) + float(moveAvg_30sec[i + 16].values()[0]) + float(moveAvg_30sec[i + 17].values()[0]) + float(moveAvg_30sec[i + 18].values()[0]) + float(moveAvg_30sec[i + 19].values()[0])) / 20; moveAvg_10min.append({time: avg10});for e in moveAvg_10min: # print(e) time = e.keys()[0]; timeList.append(time) pm25 = e.values()[0]; pm25_float = float(pm25) pm25List.append(pm25_float)# visualizationdf = pd.DataFrame({'time':timeList, 'pm2.5': pm25List})# df['pm2.5'] = df['pm2.5'].astype('float64') # convert pm2.5 string to floatdf['index_col'] = df.index;# print(df.head(5));fig, ax = plt.subplots(1,1)ax.set_xticklabels(df['time'], rotation=15, fontsize=18)sns.lineplot(x=\"index_col\", y=\"pm2.5\", data=df)plt.title(\"10 min move average\")plt.show()","link":"/2018/11/26/iot-course2018-HW6/"},{"title":"Tensorflow Notebook 1 - Python實作二元線性分類器","text":"Basic - Perceptron生物的神經元，包含樹突(Dendrites)、細胞核(Body)、軸突(Axon) Artificial Neural Network模仿神經元運作 令activation function運算每個input*weight之和若weight皆為0，加入bias避免輸出恆為0的情況 數學式:$$\\sum_{i=0}^{n} w_{i}x_{i} + b$$ Multiple Perceptron Network Input layer real valus from the data Hidden laayer layers in between input and output 3 or more layers is deep network Output layer Final estimate of the output Cost Function Notationy : true valuea : neuron’s prediction 1. Quadratic Cost$$C = \\frac{\\sum (y-a)^{2}}{n}$$ y - a即為error越大的error，則cost越大會造成learning speed下降 2. Cross Entropy$$C = \\frac{-1}{n}\\sum (ylna+(1-y)ln(1-a))$$ error越大則learning speed越快(the larger the difference, the faster the neuron can learn.) ConceptWe need to figure out how we can use our neurons(activation function) and the measurement of error (cost function) and then attempt to correct our prediction (learning) Manual Neural Network以下用python實作一個ANN，並分類一個2個label的資料集(二元線性分類器) Basic Graph “Graph”可想像成一連串nodes，n1、n2為常數輸入，n3為”operation”，繼承此class實作各種運算，例如上圖add operation，最後輸出為3 Python ImplementationOpeartion123456789101112class Operation(): def __init__(self, input_nodes=[]): self.input_nodes = input_nodes self.output_nodes = [] for node in input_nodes: node.output_nodes.append(self) _default_graph.operations.append(self) def compute(self): pass 123456class add(Operation): def __init__(self, x, y): super().__init__([x, y]) def compute(self, val_x, val_y): #overwrite self.inputs = [val_x, val_y] return val_x + val_y 123456class multiply(Operation): def __init__(self, x, y): super().__init__([x, y]) def compute(self, val_x, val_y): #overwrite self.inputs = [val_x, val_y] return val_x * val_y 123456class matmul(Operation): # matrix multiply def __init__(self, x, y): super().__init__([x, y]) def compute(self, val_x, val_y): #overwrite self.inputs = [val_x, val_y] return val_x.dot(val_y) Placeholder1234class Placeholder(): def __init__(self): self.output_nodes = []; _default_graph.placeholders.append(self) Variable123456class Variable(): def __init__(self, initial_value = None): self.value= initial_value self.output_nodes = [] _default_graph.variables.append(self) Graph123456789class Graph(): def __init__(self): self.operations = [] self.placeholders = [] self.variables = [] def set_as_default(self): global _default_graph _default_graph = self 12z = Ax + bA = 10, b = 1 Session此時需要先有post-order traverse function來查看運算是否按照正確順序，比如說這個範例需要先相乘再相加，因此定義session class 12345678910def traverse_postorder(operation): nodes_postorder = [] def recurse(node): if isinstance(node, Operation): for input_node in node.input_nodes: recurse(input_node) nodes_postorder.append(node) recurse(operation) return nodes_postorder 123456789101112131415161718192021class Session(): def run(self, operation, feed_dict={}): # feed_dict to input a batch of data nodes_postorder = traverse_postorder(operation) for node in nodes_postorder: if type(node) == Placeholder: node.output = feed_dict[node] elif type(node) == Variable: node.output = node.value else: #operation node.inputs = [input_node.output for input_node in node.input_nodes] node.output = node.compute(*node.inputs) # args if type(node.output) == list: node.output = np.array(node.output) return operation.output Usage1234567891011121314# example1g = Graph()g.set_as_default()A = Variable(10)b = Variable(1)x = Placeholder()z = add(multiply(A, x), b)sess = Session()result = sess.run(operation = z, feed_dict = {x:10})print(result) #101 123456789101112131415# example2g = Graph()g.set_as_default()# z = Ax + bA = Variable([[10, 20], [30, 40]])b = Variable([1, 2])x = Placeholder()y = matmul(A, x)z = add(y, b)sess = Session()sess.run(operation=z, feed_dict={x:10}) Perceptron type model應用此模型來做分類，從簡單的linear classification開始! Activation function先引入視覺化套件12import matplotlib.pyplot as plt%matplotlib inline 定義activation function12def sigmoid(z): return 1 / (1 + np.exp(-z)) 先看看這個function的長相 1234sample_z = np.linspace(-10, 10, 100)sample_a = sigmoid(sample_z)plt.plot(sample_z, sample_a) 建立運算(繼承Operation) 12345class Sigmoid(Operation): def __init__(self, z): super().__init__([z]) def compute(self, z_val): return 1 / (1 + np.exp(-z_val)) Feature之間的線性關係上面我們定義了神經元中的activation function，現在我們要找出feature之間的線性關係，這是我們從資料集中觀察到的結果，是人類自己需要歸納、分析的部分。(source: https://isaacchanghau.github.io/post/activation_functions/) 使用scikit-learn的內建資料集產生器 123from sklearn.datasets import make_blobsfeatures, labels = make_blobs(n_samples = 50, n_features = 2, centers = 2, random_state = 55) 畫出斑點分布1plt.scatter(features[:, 0], features[:, 1], c=labels) 注意!! 這張圖的兩軸各代表一種feature，顏色代表label 現在，我可以用肉眼觀察，y = x + 5可以很好地劃分2種斑點1234x1 = np.linspace(0, 11, 10)y1 = x1 + 5plt.scatter(features[:, 0], features[:, 1], c=labels)plt.plot(x1, y1) 接著要實際定義perceptron來構成最簡單的前饋神經網路(Feedforward Neural Network) 前饋神經網路是最早被發明的ANNThe feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network. - Wikipedia Matrix representationy = x + 5其中x,y都是feature可寫成feat2 = feat1 + 5，移項後為-feat1 + feat2 - 5 = 0矩陣表示成所以給定feat1、feat2，代入上面式子為0代表在y = x + 5線上令$f(feat1, feat2) = -feat1 + feat2 - 5$若$f(feat1, feat2) &gt; 0$表示在直線y = x + 5劃分的上半區域，即label1若$f(feat1, feat2) &lt; 0$表示在直線y = x + 5劃分的下半區域，即label2 因此稱之為linear classification 建構神經元12g = Graph()g.set_as_default() $\\sum_{i} wixi + b$1234x = Placeholder()# f(feat1, feat2) = −feat1 + feat2 − 5 w = Variable([-1, 1])b = Variable(-5) activation function:$a = f(\\sum_{i} wixi + b )$12z = add(matmul(w, x), b)a = Sigmoid(z) 再複習一次，sigmoid會給我們一個近似0或1的數，類比成神經元的行為就是當輸入訊號到達某個threshold(上圖就是0)，產生電脈衝。電脈衝沿著軸突並通過突觸傳遞到其它神經元 現在我們已經完成神經元的構造，現在我有一組features=(-8, 10)丟進這個perceptron model，看看這個model會辨識出什麼123sess = Session()sess.run(operation=a, feed_dict={x: [-8, 10]})# 0.99999773967570205 輸出一個很接近1的數，代表label = 1我們可以看到分布圖中(-8, 10)的位置就是直線上半的某點 12sess.run(operation=a, feed_dict={x: [0, -5]})# 4.5397868702434395e-05 輸出一個很接近0的數，代表label = 0","link":"/2019/01/28/Tensorflow-notebook-1/"},{"title":"CVSD 2018 Fall Nodebook","text":"Verilog基本語法Operators Bitwise Operators(每個bit都會做): ~(invert)加在兩變數中間變數之前 &amp;,|,^(XOR),~^(XNOR) ex:~m,m&amp;n Unary Operators (加在變數之前，對自己的每個bit做運算，縮至一個bit)&amp;,~&amp;,|,~|,^,~^ ex: &amp;m Arithmetic Operators: +,-(放在兩個數字中間當兩數相減、放在一個數字前面代表對他取二補數),(無號數的乘法),/,%(取餘數，這個指令不可合成) ex: m+n Logic Operators: !(not),&amp;&amp;,|| ex:!m Equality Operators(比較數值): == (數值是否相等?), != (does not equal?) ex: m==n Identity Operators(不可合成，只用在testbench): === (是否等價?多考慮X、Z的狀態), !== (是否不相同) ex: m===n Relational Operators: &lt;,&gt;,&lt;=,&gt;= Logical Shift Operators: &lt;&lt;,&gt;&gt; ex: m&lt;&lt;2，代表m左移兩個bits，相當於4(補0) Misc Operators: 條件?為真的值:為偽的值(三元運算子),{m,n}(串接),{n{m}}(m重複n次) Procedural Block: 出現always@或initialInitial block: 在testbench中給初值 always@: 當條件改變時，總是會做… 用於需要FF時posedge/negedge:正/負緣觸發，若reset看clk為synchronous，不看為asynchronous 12//下列為asynchronousalways@(posedge clk, negedge reset) 用於需要條件判斷時(合成MUX)出現在Procedure Block等號左邊的變數(output)要宣告成reg，因為這變數會因為某些條件改變而改變，所以需要存一段時間，但這不代表需要FF AssignmentContinuous assignment:在procedural block的外面，用assign的方式給值，output要宣告成wireProcedural assignment:在procedural block中給值，output要宣告成reg，又分為blocking(用=，做完才做其他的事)或non-blocking(用&lt;=，會先把右手邊都做完，等到跳出block時，一起把值吐到左手邊去)123a = b;b = a; // 結果是 a=a 123b &lt;= a; a &lt;= b; // 結果是b update成 a的舊值，a update成 b的舊值 (swap) 123CLR=#5 1; CLR=#4 0; // 結果是一行一行做 123CLR&lt;=#5 1; CLR&lt;=#4 0; // 結果是按照時間順序 Logic分成combinational(根據不同的輸入產生不同的輸出，可用讓它自己找需要的參數)和sequential logic(還會跟之前的狀態有關，看sensitive list來合FF，不能用) 邏輯判斷case…default…12345case(判斷條件) 8’b1: … default: 異常時，易判斷問題出在哪endcase if…else…123if (判斷條件) ...else if(判斷條件) ...else ... 如果條件沒寫滿會不知道該怎麼辦，只好保持不動，產生latch如果要告訴合成軟體說這就是所有會發生的情形，可以如下Case(判斷條件) // synopsys fullcase Ch1 module name不要取數字，不要用特殊符號 變數前面的是vector，代表wordlength變數後面的是number，代表有幾個 預設ns Ch2 tri-state buffer盡量不要用，合成和P&amp;R時會錯 優先順序由上到下 unsized constant在沒宣告時預設32 bits combinational loop可簡化logic but CAD tool不支援，因為會被視為asynchronous而不斷執行，很難去預測答案 always @ block用在1.邏輯判斷2.希望合成出DFF(Sequential circuit) Ch3 timing包含兩種(1)delay:元件的特性 (2)clock period: 有多少的時間給我運算(兩個FF中間有多少時間) 根據clock period的timing constraint，會去調整standard cell的size，以符合timing的限制ex:希望加法器變快，會把standard cell的size加大，面積加大之後電路操作的速度會變快，delay會變小，就比較有機會符合clock period的規格 所以圖中講的speed會隨delay而異但由於clock period的限制，我們可能會調整面積去meet timing constraint，所以某種程度上，圖中的speed也會隨著調整clk period而改變 跟don’t care比較永遠永遠都會是錯的 從design compiler得到的timing，是寬鬆的設計，如果要測量電路可以跑多快，可以一直把timing constraint(sdc檔中)往下降，直到出現negative slack為止 在所有的輸出加register比較好，這樣可以確保三個電路會在同一個時間內算完，這樣電路會比較好做測試，因為從波形上可以看到穩定的數字，不會出現glitch，但需要看的點再加register就好，因為這會讓latency變差 DC不會optimize電路，只會optimize元件，如果沒做pipeline，電路可能會來不及算完現在的tool會自動幫忙移動FF，只要把設計寫對之後加FF，tool就會做retiming 把critical path 和其他電路分開，才能分別optimize 如果case沒寫滿會合出latch，這易受noise影響，因為latch是combinational的電路，只要輸入改變，輸出就會跟著改變 [p.37] [p.38] signed會告訴compiler，MSB代表signed bit 轉成signed的三種可能方式(1)宣告時加signedex: wire signed [8:0] c;(2)在tb的$display()內，變數前加一個$signed(3)在tb的$display()內，數字前加一個sex: 9'sd1 如果變數沒有宣告成signed bit，會預設為unsigned比較時，若比較對象出現unsigned的比較，要轉為unsigned signed跟unsigned的mapping從0開始由小到大先寫正整數，寫完之後接著由小到大寫負數 Q:4 bits的範圍由幾到幾?16個數字中，一個被0用掉，剩下15個分給正和負，負數會比正數多一個(因為正數有一個給0使用)4bits =&gt; -8~+7 如果debug要看陣列內的訊號，要加p.42最下面的兩行指令 如果合成後看到type有出現latch，要回頭把RTL的code改掉 [p.75] 圖中的datafile是design好要餵給testbench吃test pattern的檔案 @代表值要被放到哪個address，投影片中的address是採用16進制ex:@100 =&gt; 1×16^2^+0×16^1^+0×16^0^=256 VCD(value change dump) format: 所有訊號轉變的波形檔，檔案較大FSDB(fast signal database) format: 會做編碼和去掉不重要的，檔案較小如果要看二維陣列的訊號，要加+mda，這是所有memory波形檔 [p.84] post-sim時要include sdf中，這樣才會看時間有沒有violatesdf檔是在合成之後，會記錄時間資訊的檔案 Ch4 Latency是一個data進來到完成運算所需要的時間Throughput是單位時間內可完成的運算量 pipeline是插FF，如果在一個path插一個FF，切一刀之後delay不會變一半，因為插入FF的set-up time也要考慮 電路的資料流是有方向性的，要確保cut-st上資料的流向要一致 多組元件平行進行，增加了Throughput，但要確保input能夠吃下這樣的loading Retiming: tool會自動讓cell在FF間做移動 N~node~: node的數量C~L~: 後端tool才能optimizeV^2^~dd~: 這受限於製程，不能改只能調整clock的頻率 以下的方法可以減少Power Consumption(1)讓電路正常運作就好(2)不要用的時候會關掉clk(3)平常沒在運作時可以讓clk的速度變慢(4)做不重要的動作時，可以把電壓降低(5)減少不必要的切換，例如counter數完時，讓他停住不要再跳 glitch發生時，可以用FF去擋 Ch5-1 考試重點 set-up time: 這一筆資料不能太慢抵達 FF在trigger時，存到rigister前，不能改變(因為clk不會立刻跳，電路不能跑太快) T~clk~ &gt; T~CLKtoQ~ + T~data~(combinational) + T~setup~ - T~skew~ hold time: 下一筆的資料不能太快來，否則會data feedthrogh，tool會自動插delay修正 T~hold~ &lt; T~CLKtoQ~ + T~data~ - T~skew~ STA Link set-up time 的T~CLKtoQ~是從clk起來，一直到出現stable的輸出所需要的時間hold time 的T~CLKtoQ~是從clk起來，一直到出現輸出所需要的時間，若要更嚴謹地不被下一筆資料做影響，下一筆的輸出可以是unstable的另外set-up time 要取最長的path，而hold time要取最短的path做檢查 成因：因為clock不是理想的，寫到FF內的暫存器時，不是一瞬間的 =&gt; set-up time。set-up time: 這一筆資料被寫到暫存器所需要的時間，所以這一筆資料不能來的太慢(slow.db是用來確認setup time)。成因：因為clock不是理想的，第一級的暫存器開關不會立刻關掉，下一筆資料如果來太快，會被寫入暫存器內部，所以資料需要維持一段時間不能變動 =&gt; hold timehold time: 下一筆資料來之前所需要維持的時間，所以下一筆資料不能來的太快(fast.db是用來確認有無hold time violation)。 set-up time 在檢查時要取==最慢==的path.hold time 要取==最快==的path. Q:如何解決violationA: set-up time : 做retiming(調整FF的位置)，平均分攤每兩級FF之間的combinational logic time 或者是調大clock。hold time : 插delay buffer。直到滿足兩個不等式為止。 skew time：因走線有寄生電容，線越長Clock越晚到，所以相較之下，Data會來得比較快這一筆資料來得比較快，對setup time有利，所以不等式是用加的。下一筆資料也來的比較快，所以對hold time不利，所以不等式是用減的。 Clock Skew: Implication on Timing clock skew又稱作clock uncertainty，clock因為繞線以及layout等於因導致抵達各個register時的時間不一。但是在STA階段無法準確知道哪個register收到的clock是快的或是慢的，所以design compiler在分析static timing analysis時，在分析setup time或hold time時必須一律以worst case來做分析。“邏輯所能使用之clock period皆減少”T~CLKtoQ~ + T~data~(combinational) + T~setup~ ==+== T~skew~T~hold~ &lt; T~CLKtoQ~ + T~data~ - T~skew~ 考試重點 Technology library:.db(給人看)、.lib(給機器看)，紀錄在不同操作環境下的cell的timing的資訊，提供給dc(design compiler)做合成 PVT:Process Voltage Temperature typical.db:中速製程 slow.db:慢速製程，分析set-up time fast.db:快速製程，分析hold-time CPU跑超頻時=&gt;加大電壓或加液態氮(降溫)、電流取決於mobility、threshold voltage，會決定電路的操作速度而μ、V~t~隨溫度改變的趨勢不同，並不是一直降溫都會有用 design compiler會把.db和.lib互轉 [p.6] Design Ware Library 像合成的函式庫HDL compiler會吃進RTL Design和Design Ware Library，轉成high level的gate level(例如:一群FF、MUX、加法器、乘法器、ALU)Design compiler會吃進Design constraint和HDL Compiler給他的檔案，還有Design Ware Library給他的，輸出Optimized Gate-level Netlist，這個步驟才有吃進process的資訊 檔案的整理 Design Ware Library 像合成的函式庫HDL compiler會吃進RTL Design和Design Ware Library，轉成high level的gate level(例如:一群FF、MUX、加法器、乘法器、ALU)Design compiler會吃進Design constraint和HDL Compiler給他的檔案，還有Design Ware Library給他的，輸出Optimized Gate-level Netlist，這個步驟才有吃進process的資訊 .db / .lib: Technology library:.db(給人看)、.lib(給機器看)，紀錄在不同操作環境下的cell的timing的資訊，提供給dc(design compiler)做合成用 .synopsys_dc.setup:設定環境的library，提供給DC做合成ex:設定要去哪個(search) path尋找SRAM或I/O pad的cell、還有target (technology) library 為哪個操作環境的db，有設定才會認得SRAM和I/O pad，才會在area report反映在Macro的面積 .sdc檔(synopsys design constraints) ，記載一些合成前的設定，提供給DC做合成裡面有的內容如clk的period給多長來操作，clk長怎樣、不明朗的範圍多大、hold time多少、哪些block不要動且要設為ideal、一些會影響delay的設定、操作環境為何、set maximal area constraint、cost是以什麼考量為優先的、有沒有設定wire load model、線的推動力怎樣、input/output delay多大 .sdc、main.tcl都是屬於自動化的腳本以上都是提供給DC做合成用，以下是合成完後要提供給ncverilog做模擬用 _syn.v(Gate level netlist): 根據DC compiler吃的.sdc、HDL compiler提供的high gate level netlist、designware library的參數去合成gate level的netlist，提供給ncverilog做模擬用 .sdf(Standard Delay format)，紀錄一些時間的資訊，給ncverilog做模擬用應該是根據合成前.sdc中的設定，萃取出一些的時間資訊，例如input delay、output delay、wire load model的delay為何、hold time為多少，然後提供給ncverilog做模擬 tsmc13.v(Process Library):提供合成後verilog netlist map 到 cell的 behavior 之後timing的資訊，提供給ncverilog做模擬用 .ddc:DC合成完後會儲存的工作檔 [p.7] [p.13].synopsys_dc.setup 環境設定的library，讀取DV和DC的指令然後開始執行，一定要跟DV/DC檔在同一個資料夾 [p.13][p.15] .gds layout 考試重點 可以把要做的事情寫成文字檔(script)，這會跟開dv按按鍵的步驟都有對應P&amp;R要用script跑，通常在synthesis叫出GUI的機會不高，除非要找critical path去修negative slack，或者是要看合出來的東西是否正確 12345678910111213141516171819202122232425262728293031323334353637# main.tcl# Import Designread_file -format verilog \"./CLE.v\"current_design [get_designs CLE]linksource -echo -verbose ./CLE_DC.sdc# Compile Designcurrent_design [get_designs CLE]set high_fanout_net_threshold 0uniquifyset_fix_multiple_port_nets -all -buffer_constants [get_designs *]compile # Output Designcurrent_design [get_designs CLE]remove_unconnected_ports -blast_buses [get_cells -hierarchical *]set bus_inference_style {%s[%d]}set bus_naming_style {%s[%d]}set hdlout_internal_busses truechange_names -hierarchy -rule verilogdefine_name_rules name_rule -allowed {a-z A-Z 0-9 _} -max_length 255 -type celldefine_name_rules name_rule -allowed {a-z A-Z 0-9 _[]} -max_length 255 -type netdefine_name_rules name_rule -map {{\"\\\\*cell\\\\*\" \"cell\"}}define_name_rules name_rule -case_insensitivechange_names -hierarchy -rules name_rulewrite -format ddc -hierarchy -output \"CLE_syn.ddc\"write -format verilog -hierarchy -output \"CLE_syn.v\"write_sdf -version 2.1 CLE_syn.sdf 考試重點 名詞定義 看圖 [p.40] Ch5-2 也可以把上面的指令寫在.sdc檔 dc_shell&gt; set_clock_uncertainty 0.1 [get_clocks clk] skew會讓data來得相較clock來說比較快，這一筆資料來得快，對set-up time有利，下一筆資料來得快，對hold time不利 [p.5] clock uncertainty會讓能算的時間下降 false path代表這條路徑不可能會發生，要了解電路的樣子才能設，這是要DC不要檢查這條路徑 Clock gatingLow power designEN=1時為一般clk trigger的FF；EN=0時，module被gating掉，不會操作Enable訊號一定是透過logic來的，可能會有glitchtool合法化了一個clk gating cell，由Enable控制module是否會被打開 clk gating cell有兩種合成的方法，楊老師Lab推薦第一種作法 FF中，寫if而沒有else，在合成時會利用latch cell把module給關掉 assign glk= clk &amp; EN ， EN=0時，clk不會跳，但要在dc_shell中輸入指令，確保被換成clk gating cell area與process有關，算gate count時，要把logic都normalize成nand gate或nor gate dc_shell&gt; set_max_area 0希望達到的目標最佳化問題有目標函數和限制函數，我們希望area越小越好，所以目標可以直接訂成0，DC會努力幫我們去壓面積，因為這不是我們的限制，所以不用擔心會過不了但有時候壓面積會讓timing constraint過不了，兩者常常互相衝突 這一頁投影片這個與Physical design的DRC不同除了timing以外，這些參數的設定也會影響到delay的演算法因為delay的算法是透過standard cell calibre在某個區間，透過查表算出來的，有一個較準的範圍，所以要讓電路操作在校準的環境 一般我們performance先看timing，再看area，之後才會看power有多大可以用 set_cost_priority來自訂他们的優先權 設dont_touch: RTL就不會在動可能已經有額外利用DC花了很多時間把某些block給合成好，不希望DC再去更動合成好的block 這會把module間的邊界打破，可以做到跨模組的優化，但會花很多時間，在職場上time to market也很重要 這比較少用，可以針對每個實體，create一個design，大家會有各自的counter 以上的步驟都是寫在.sdc檔中 如果出現負的slack，代表電路跑太慢，要切pipeline或者回去修正RTL的code 圖中的橫軸是slack的長度，縱軸是path的數量(1)設計要盡量讓所有的path都靠近0，切pipeline要把大家的時間切得差不多(2)調整完之後再改變.sdc檔中的clk cycle，把clk往下壓(3)重複(1)、(2)的動作直到不能再把clk壓下去為止(4)如果不知道怎麼切pipeline，有最後一招可以去解slack，但記得這一招只能用一次可以插一堆registers之後，下optimize_register的指令，讓DC幫我們優化 optimize後的名字可能會看不懂，因為會產生很多設計以外的東西，且design會比較大(area buffer cost) 有set wire load model時，會有Net Interconnect area和Total area(Total cell area + Net Interconnect area)，這兩項不用考慮，我們只要負責優化Combinational area(adder,乘法器、MUX…)和Noncombinational area(Sequential FF)Macro/Black Box area指得是memory的面積 ex:SRAM的面積 合成時就要考慮I/O pad模擬時要有I/O pad的behavior檔 slow.lib是給standard cell吃的環境tpz013g3wc.lib是外圍padrom_1024×4_t13_slow_syn.lib是memory的library，裡面包含timing、power的資訊，透過DC可以轉成.db檔給機器看 HW3中的CLE_DC.sdc .ddc檔是關design compiler之後的工作站存檔 tsmc13.v是 CORE的behavior model，是用來跑模擬用的 Ch6 Fault Coverage是偵測出fault的數量與所有可能出現fault數量的比值 [p.29] and的特性，兩個輸入中若有一個是0，結果應該是0；若兩個輸入都是1，結果應該是1，當違反and gate的原則，就代表有錯誤，那這個fault就能夠被看到測試的兩大重點 Activate: 輸入的pattern要能夠讓fault被反映出來(選與fault相反的值，藉由這個特性可以回推部分test pattern該有的值) Propagate: fault要能夠反映到輸出，不能被其他的logic gate擋下來(藉由這個特性可以回推其他test pattern該有的值) 應用and gate 和 or gate的天性，我們可以在activate fault之後，做implication，推回test pattern應該為多少 如下圖，stuck at fault 就是將一 input 卡在錯誤的值，看 output 是否能夠被偵測到(與原先 pattern 應得結果不同)，若有偵測到，表示 detect fault。 以下圖為例， s-a-0(D) =&gt; 我們首先要能 activate 它，就必須假設它為1，再往回推要得到1所需要的條件(A=1、B=1)。 為了能將錯誤表現出來，我們希望它能影響到 output 的值，也就是 propagation，為了能將 0 的錯誤推至 output 需要 E=1 ，E=0 的話 D 不論為 0、1 都無法表現。 (P.29 與 P.39 為考試重點) 下圖也是依照兩個原則 activation : 7 條 wire 可以插 stuck at fault 。 propagation : 看看你插的 fault 會不會改變 output 原先 pattern 正確的值。注意 : 只需考慮 single stuck at fault ，一次只考慮一個 fault，其他假設都正確。 &lt;!– –&gt; Ch7 四種data pathStart Point可能是Clock pin(CLK port)或者是PI(primary input)End Point可能是PO(primary output)或者是data input pin(D)所以會有四種可能的Path CLK port到FF的data input(D)CLK port到POPI到FF的data input(D)PI到PO 有幾個Start points?PI: A,B,C有三個CLK port有四個共七個 有幾個End points?PO:O,Q,P有三個FF的data input port有四個共七個 有幾條timing path?底下圖中紅色的1+1+3+1+4+4+2=16 &lt;考&gt;到這個點為止需要多少時間由前面往後算所有要花的時間取最大值每個edge都可以定義delay，delay的累積就是arrival timemultifanin要取大的delay，例如:中間NOR gate的input分別為0.65和0.7，要取比較慢的，所以是0.7+0.2 &lt;考&gt;這個點還有多少時間可以使用所有剩下的時間取最小值Spec.的要求要接到後面的電路，會被要求多少時間之前資料必須要被準備好由後面往前算希望PO的vertex在時間1.5的時候要收到資料由後往前推時，如果有兩個以上的剩下時間，要取比較小的，再繼續往前把每一段需要的時間一直消耗，就可以算出每個vetex的required time是多少 &lt;考&gt;required time - arrived time 可以得到slack graph如果slack是負的，就是critical path，要讓slack&gt;0，才不會有timing violation 兩種方式造成clock latency off chip cause: 時脈產生器到晶片過程造成的delay機台上有石英震盪器，到晶片的時間model出的行為 on chip cause: 線長不同、擺置線寬不同，造成sequential吃到的clk不同 取最大的跟最小的相減，代表clk可能會浮動的範圍就算使用H tree的擺法thermal noise造成clk到每一顆FF的時間略有不同波型起來的地方可能會早一點或晚一點到下去的地方可能早一點或晚一點下去這些可以靠tool模擬出來 哪些在設計中是false path，在SAT計算中永遠不會發生電路中的某個path平常不會被操作到ex:reset的block平常不會被觸發ex:有個訊號不再被需要了(input vector永遠不會再觸發某一條path的功能)，別的vertex看那一條會是constantex:這個path不會再影響其他node甚至POex:combinational loop不應該在電路中發生，所以被設為false(DC目前還沒有辦法做分析)cf.跟之前所說的，set false path 故意設定false讓DC compiler不要去估timing analysis，不太一樣 &lt;SOCV有教&gt;setup-time 吃到combinational circuit的訊號後sequential circuit需要花一段時間才會把值propagate到output，必須要把這些結果計算起來圖中沒事是On time前面propagate太久壓縮到setup time 這一筆資料在path上的值不可以來得太慢，是剩下的時間扣掉實際到達的時間 Hold time: combinational circuit算出值之後要被Sequenial circuit寫入，所以不能太快改變 反過來減，下一筆資料，經過path不可以算得太快，值要維持得夠久，所以是arrival time減掉required time 以下為考古的問答題問答1: interconnect area如果是0，如何解決??就是沒設wire load，在.sdc檔 set_wire_load_model 有關 Net Interconnect area是0: 在set design constraint(.sdc)檔案中，如果沒有設定set wire load model，那cell間的走線面積就不會考慮，所以interconnect area會是0set_wire_load_model -name tsmc13_wl10 -library slow 1234567891011121314151617181920212223# CLE_DC.sdc# You can only modify clock period set cycle 10 ;#clock period defined by designer#don&apos;t modify the following partcreate_clock -period $cycle [get_ports clk]set_dont_touch_network [all_clocks]set_fix_hold [all_clocks]set_clock_uncertainty 0.1 [all_clocks]set_clock_latency 0.5 [all_clocks]set_ideal_network [get_ports clk]#Don&apos;t touch the basic env setting as belowset_input_delay 1 -clock clk [remove_from_collection [all_inputs] [get_ports clk]]set_output_delay 1 -clock clk [all_outputs] set_load 1 [all_outputs]set_drive 1 [all_inputs]set_operating_conditions -max_library slow -max slowset_wire_load_model -name tsmc13_wl10 -library slow set_max_fanout 6 [all_inputs] 問答2: 為什麼Macro/Black Box area是0?因為環境設定檔中，.synopsys_dc.setup內的SRAM路徑不對，DC找不到SRAM的library，認不得所以沒吃到Macro的面積search_path會去搜尋Macro cell的gate netlist path在哪裡target_library是放置Macro cell 的technology library的path在哪裡，透過target_library，我們會把link_library link到Macro的cell，這樣就能認得Macro的cell 12345678# .synopsys_dc.setupset search_path &quot;. /home/raid7_2/course/cvsd/CBDK_IC_Contest/CIC/SynopsysDC/db ./Memory/sram_1024x8_t13 $search_path&quot;set target_library &quot;slow.db \\ typical.db fast.db sram_1024x8_t13_slow_syn.db \\ &quot;set link_library &quot;* $target_library dw_foundation.sldb&quot; 123456789101112131415161718192021222324252627282930313233343536# .synopsys_dc.setup# 1. Modify this file to fit your own environment.# 2. Copy this file synopsys_dc.setup to .synopsys_dc.setup# and put this file in tool&apos;s invoking directory or your home directory.# set company {NTUGIEE}set designer {Student}set search_path &quot;. /home/raid7_2/course/cvsd/CBDK_IC_Contest/CIC/SynopsysDC/db ./Memory/sram_1024x8_t13 $search_path&quot;set target_library &quot;slow.db \\ typical.db fast.db sram_1024x8_t13_slow_syn.db \\ &quot;set link_library &quot;* $target_library dw_foundation.sldb&quot;set symbol_library &quot;tsmc13.sdb generic.sdb&quot;set synthetic_library &quot;dw_foundation.sldb&quot;set default_schematic_options {-size infinite}set hdlin_translate_off_skip_text &quot;TRUE&quot;set edifout_netlist_only &quot;TRUE&quot;set verilogout_no_tri trueset hdlin_enable_presto_for_vhdl &quot;TRUE&quot;set sh_enable_line_editing trueset sh_line_editing_mode emacshistory keep 100alias h historyset bus_inference_style {%s[%d]}set bus_naming_style {%s[%d]}set hdlout_internal_busses truedefine_name_rules name_rule -allowed {a-z A-Z 0-9 _} -max_length 255 -type celldefine_name_rules name_rule -allowed {a-z A-Z 0-9 _[]} -max_length 255 -type netdefine_name_rules name_rule -map {{&quot;\\\\*cell\\\\*&quot; &quot;cell&quot;}} 問答3: timing report是讓critcal path可以過timing的constraints，如果過了5ns的constraints，把testbench改成4ns使否有機會meet constraints?有機會，如果測資走的不是critical path，那就有機會通過限制 問答4: Why we only focus on total cell area instead of total area?因為我們能夠掌握的只有RTL的design，而他會影響的面積是Cominational、Buffer、Non-combinational的area，wire的面積不是我們能決定的，而且這個面積在P&amp;R之後才會比較準確 問答5: Why the total cell area will be underestimated if we don’t set wire load model?因為沒有設定wire model，那就不會考慮wire的delay，所以timing constraints會比較好通過若今天把wire delay考慮回來，那timing constraints的條件會變得比較嚴苛，所以cell的面積必須要加大，讓操作速度變得比較快，才能通過constraints 問答6: 如果今天把.sdc中的clock period設小，area會怎麼改變?由於clock period變小，timing constraints的條件變嚴苛，需要更大的硬體面積才能通過timing的限制，所以area會變大(area越大，delay越小) 問答7: 我們需要synthesis rom_1024×4_t13.v with DUT.v(我們設計的module)嗎?不用，因為.v檔已經有提供ncverilog模擬timing delay的資訊，不需要再合成另外我們已經有合成後的.gds檔(layout檔)，所以不用再合成","link":"/2018/11/12/cvsd-note/"}],"tags":[{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Scikit-learn","slug":"Scikit-learn","link":"/tags/Scikit-learn/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Decision Tree","slug":"Decision-Tree","link":"/tags/Decision-Tree/"},{"name":"Random Forest","slug":"Random-Forest","link":"/tags/Random-Forest/"},{"name":"K-Means","slug":"K-Means","link":"/tags/K-Means/"},{"name":"Linear Regression","slug":"Linear-Regression","link":"/tags/Linear-Regression/"},{"name":"Logistic Regrassion","slug":"Logistic-Regrassion","link":"/tags/Logistic-Regrassion/"},{"name":"KNN","slug":"KNN","link":"/tags/KNN/"},{"name":"Raspberry Pi","slug":"Raspberry-Pi","link":"/tags/Raspberry-Pi/"},{"name":"MJPEG-Streamer","slug":"MJPEG-Streamer","link":"/tags/MJPEG-Streamer/"},{"name":"JavaScript","slug":"JavaScript","link":"/tags/JavaScript/"},{"name":"Promise","slug":"Promise","link":"/tags/Promise/"},{"name":"ES6","slug":"ES6","link":"/tags/ES6/"},{"name":"SVM","slug":"SVM","link":"/tags/SVM/"},{"name":"Minecraft","slug":"Minecraft","link":"/tags/Minecraft/"},{"name":"Google Compute Engine","slug":"Google-Compute-Engine","link":"/tags/Google-Compute-Engine/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Node.js","slug":"Node-js","link":"/tags/Node-js/"},{"name":"Verilog","slug":"Verilog","link":"/tags/Verilog/"},{"name":"Facebook","slug":"Facebook","link":"/tags/Facebook/"},{"name":"Ngrok","slug":"Ngrok","link":"/tags/Ngrok/"},{"name":"base64","slug":"base64","link":"/tags/base64/"},{"name":"Firebase","slug":"Firebase","link":"/tags/Firebase/"},{"name":"Tensorflow","slug":"Tensorflow","link":"/tags/Tensorflow/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"Jupyter","slug":"Jupyter","link":"/tags/Jupyter/"},{"name":"IoT","slug":"IoT","link":"/tags/IoT/"},{"name":"LoRa","slug":"LoRa","link":"/tags/LoRa/"},{"name":"NodeRED","slug":"NodeRED","link":"/tags/NodeRED/"},{"name":"MQTT","slug":"MQTT","link":"/tags/MQTT/"},{"name":"AWS","slug":"AWS","link":"/tags/AWS/"},{"name":"matplotlib","slug":"matplotlib","link":"/tags/matplotlib/"},{"name":"DynamoDB","slug":"DynamoDB","link":"/tags/DynamoDB/"}],"categories":[]}